{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40749661",
   "metadata": {},
   "source": [
    "# 08 - Fabric Ontology REST API Client\n",
    "\n",
    "**Epic:** F5 - Fabric Ontology Integration  \n",
    "**Feature:** F5.2 - Fabric Ontology REST API Client  \n",
    "**Priority:** P1\n",
    "\n",
    "## Purpose\n",
    "\n",
    "Upload the generated Ontology definition to Fabric using the Ontology REST API. Creates a new Ontology item or updates an existing one.\n",
    "\n",
    "## Input\n",
    "\n",
    "- Ontology definition JSON from `Files/ontology_definitions/` (output of notebook 07)\n",
    "\n",
    "## Output\n",
    "\n",
    "- Created/updated Ontology item in Fabric workspace\n",
    "- Ontology ID saved to `Files/config/ontology_config.json`\n",
    "\n",
    "## API Operations\n",
    "\n",
    "| Operation | Endpoint | Purpose |\n",
    "|-----------|----------|----------|\n",
    "| Create Ontology | POST /v1/workspaces/{id}/ontologies | Create new Ontology item |\n",
    "| Get Definition | POST /ontologies/{id}/getDefinition | Retrieve current definition |\n",
    "| Update Definition | POST /ontologies/{id}/updateDefinition | Push entity types, properties, relationships |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97b4bd6",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2c6290",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "import requests\n",
    "from datetime import datetime\n",
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0bb35c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fabric notebookutils for authentication\n",
    "from notebookutils import mssparkutils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092952b0",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2119d4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fabric API configuration\n",
    "FABRIC_API_BASE = \"https://api.fabric.microsoft.com\"\n",
    "FABRIC_API_VERSION = \"v1\"\n",
    "\n",
    "# Ontology configuration\n",
    "# Name must: start with letter, <90 chars, only letters/numbers/underscores\n",
    "ONTOLOGY_DISPLAY_NAME = \"RDF_Translated_Ontology\"\n",
    "ONTOLOGY_DESCRIPTION = \"Auto-generated ontology from RDF translation pipeline\"\n",
    "\n",
    "# Paths\n",
    "DEFINITIONS_DIR = \"/lakehouse/default/Files/ontology_definitions\"\n",
    "CONFIG_DIR = \"/lakehouse/default/Files/config\"\n",
    "\n",
    "# Retry configuration\n",
    "MAX_RETRIES = 3\n",
    "RETRY_DELAY_SECONDS = 5\n",
    "LRO_POLL_INTERVAL_SECONDS = 2\n",
    "LRO_MAX_WAIT_SECONDS = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da38a71",
   "metadata": {},
   "source": [
    "## Get Workspace and Lakehouse IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd62e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get current workspace and lakehouse context from Fabric\n",
    "workspace_id = mssparkutils.runtime.context.get(\"currentWorkspaceId\")\n",
    "\n",
    "# Get the default lakehouse ID from the attached lakehouse\n",
    "# Method 1: Try to get from spark config (most reliable)\n",
    "try:\n",
    "    lakehouse_id = spark.conf.get(\"trident.lakehouse.id\")\n",
    "except Exception:\n",
    "    lakehouse_id = None\n",
    "\n",
    "# Method 2: List lakehouses and get the default one\n",
    "if not lakehouse_id:\n",
    "    try:\n",
    "        lakehouses = mssparkutils.lakehouse.list()\n",
    "        if lakehouses:\n",
    "            # Get first lakehouse (the attached one)\n",
    "            lakehouse_id = lakehouses[0].id\n",
    "    except Exception:\n",
    "        lakehouse_id = None\n",
    "\n",
    "# Method 3: Get from default lakehouse spark config\n",
    "if not lakehouse_id:\n",
    "    try:\n",
    "        lakehouse_id = spark.conf.get(\"spark.lakehouse.default.id\", None)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "if not lakehouse_id:\n",
    "    raise ValueError(\n",
    "        \"Could not determine lakehouse ID. \"\n",
    "        \"Please ensure a lakehouse is attached to this notebook.\"\n",
    "    )\n",
    "\n",
    "print(f\"Workspace ID: {workspace_id}\")\n",
    "print(f\"Lakehouse ID: {lakehouse_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089c955f",
   "metadata": {},
   "source": [
    "## Authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a927ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fabric_token() -> str:\n",
    "    \"\"\"\n",
    "    Get Entra ID token for Fabric API using notebookutils.\n",
    "    Uses the user's identity or workspace identity.\n",
    "    \"\"\"\n",
    "    # Get token for Fabric API scope\n",
    "    token = mssparkutils.credentials.getToken(\"https://api.fabric.microsoft.com\")\n",
    "    return token\n",
    "\n",
    "\n",
    "def get_headers() -> dict:\n",
    "    \"\"\"\n",
    "    Get HTTP headers with authorization token.\n",
    "    \"\"\"\n",
    "    token = get_fabric_token()\n",
    "    return {\n",
    "        \"Authorization\": f\"Bearer {token}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "\n",
    "# Test authentication\n",
    "try:\n",
    "    token = get_fabric_token()\n",
    "    print(f\"Successfully obtained token (length: {len(token)})\")\n",
    "except Exception as e:\n",
    "    print(f\"Authentication failed: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65fb005",
   "metadata": {},
   "source": [
    "## API Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7504a8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def api_request(\n",
    "    method: str,\n",
    "    endpoint: str,\n",
    "    data: Optional[dict] = None,\n",
    "    params: Optional[dict] = None\n",
    ") -> requests.Response:\n",
    "    \"\"\"\n",
    "    Make an API request with retry logic.\n",
    "    \n",
    "    Args:\n",
    "        method: HTTP method (GET, POST, PATCH, DELETE)\n",
    "        endpoint: API endpoint (relative to FABRIC_API_BASE)\n",
    "        data: Request body (dict)\n",
    "        params: Query parameters\n",
    "    \n",
    "    Returns:\n",
    "        Response object\n",
    "    \"\"\"\n",
    "    url = f\"{FABRIC_API_BASE}/{endpoint}\"\n",
    "    \n",
    "    for attempt in range(MAX_RETRIES):\n",
    "        try:\n",
    "            headers = get_headers()\n",
    "            \n",
    "            response = requests.request(\n",
    "                method=method,\n",
    "                url=url,\n",
    "                headers=headers,\n",
    "                json=data,\n",
    "                params=params,\n",
    "                timeout=60\n",
    "            )\n",
    "            \n",
    "            # Check for retryable errors\n",
    "            if response.status_code == 429:  # Rate limited\n",
    "                retry_after = int(response.headers.get(\"Retry-After\", RETRY_DELAY_SECONDS))\n",
    "                print(f\"Rate limited. Waiting {retry_after} seconds...\")\n",
    "                time.sleep(retry_after)\n",
    "                continue\n",
    "            \n",
    "            if response.status_code >= 500:  # Server error\n",
    "                print(f\"Server error {response.status_code}. Retrying in {RETRY_DELAY_SECONDS}s...\")\n",
    "                time.sleep(RETRY_DELAY_SECONDS)\n",
    "                continue\n",
    "            \n",
    "            return response\n",
    "            \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Request failed (attempt {attempt + 1}/{MAX_RETRIES}): {e}\")\n",
    "            if attempt < MAX_RETRIES - 1:\n",
    "                time.sleep(RETRY_DELAY_SECONDS)\n",
    "            else:\n",
    "                raise\n",
    "    \n",
    "    return response\n",
    "\n",
    "\n",
    "def wait_for_lro(operation_url: str) -> dict:\n",
    "    \"\"\"\n",
    "    Wait for a long-running operation to complete.\n",
    "    \n",
    "    Args:\n",
    "        operation_url: URL from Location header of 202 response\n",
    "    \n",
    "    Returns:\n",
    "        Final operation result\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    while time.time() - start_time < LRO_MAX_WAIT_SECONDS:\n",
    "        headers = get_headers()\n",
    "        response = requests.get(operation_url, headers=headers, timeout=60)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            result = response.json()\n",
    "            status = result.get(\"status\", \"Unknown\")\n",
    "            \n",
    "            if status in [\"Succeeded\", \"Completed\"]:\n",
    "                print(f\"Operation completed successfully\")\n",
    "                return result\n",
    "            elif status in [\"Failed\", \"Cancelled\"]:\n",
    "                error = result.get(\"error\", {})\n",
    "                raise Exception(f\"Operation {status}: {error}\")\n",
    "            else:\n",
    "                print(f\"Operation status: {status}\")\n",
    "        \n",
    "        time.sleep(LRO_POLL_INTERVAL_SECONDS)\n",
    "    \n",
    "    raise TimeoutError(f\"Operation timed out after {LRO_MAX_WAIT_SECONDS} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f686ce",
   "metadata": {},
   "source": [
    "## Ontology API Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d62bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_ontologies(workspace_id: str) -> list:\n",
    "    \"\"\"\n",
    "    List all ontologies in a workspace.\n",
    "    \"\"\"\n",
    "    endpoint = f\"{FABRIC_API_VERSION}/workspaces/{workspace_id}/ontologies\"\n",
    "    response = api_request(\"GET\", endpoint)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        return response.json().get(\"value\", [])\n",
    "    else:\n",
    "        print(f\"Failed to list ontologies: {response.status_code}\")\n",
    "        print(response.text)\n",
    "        return []\n",
    "\n",
    "\n",
    "def get_ontology(workspace_id: str, ontology_id: str) -> Optional[dict]:\n",
    "    \"\"\"\n",
    "    Get ontology metadata by ID.\n",
    "    \"\"\"\n",
    "    endpoint = f\"{FABRIC_API_VERSION}/workspaces/{workspace_id}/ontologies/{ontology_id}\"\n",
    "    response = api_request(\"GET\", endpoint)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def find_ontology_by_name(workspace_id: str, display_name: str) -> Optional[dict]:\n",
    "    \"\"\"\n",
    "    Find an ontology by display name.\n",
    "    \"\"\"\n",
    "    ontologies = list_ontologies(workspace_id)\n",
    "    for ont in ontologies:\n",
    "        if ont.get(\"displayName\") == display_name:\n",
    "            return ont\n",
    "    return None\n",
    "\n",
    "\n",
    "def create_ontology(workspace_id: str, display_name: str, description: str = \"\") -> dict:\n",
    "    \"\"\"\n",
    "    Create a new Ontology item in the workspace.\n",
    "    \"\"\"\n",
    "    endpoint = f\"{FABRIC_API_VERSION}/workspaces/{workspace_id}/ontologies\"\n",
    "    \n",
    "    data = {\n",
    "        \"displayName\": display_name,\n",
    "        \"description\": description\n",
    "    }\n",
    "    \n",
    "    response = api_request(\"POST\", endpoint, data=data)\n",
    "    \n",
    "    if response.status_code == 201:\n",
    "        result = response.json()\n",
    "        print(f\"Created ontology: {result.get('id')}\")\n",
    "        return result\n",
    "    elif response.status_code == 202:\n",
    "        # Long-running operation\n",
    "        operation_url = response.headers.get(\"Location\")\n",
    "        if operation_url:\n",
    "            return wait_for_lro(operation_url)\n",
    "    \n",
    "    print(f\"Failed to create ontology: {response.status_code}\")\n",
    "    print(response.text)\n",
    "    raise Exception(f\"Create ontology failed: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2e56c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ontology_definition(workspace_id: str, ontology_id: str) -> dict:\n",
    "    \"\"\"\n",
    "    Get the current ontology definition.\n",
    "    \"\"\"\n",
    "    endpoint = f\"{FABRIC_API_VERSION}/workspaces/{workspace_id}/ontologies/{ontology_id}/getDefinition\"\n",
    "    response = api_request(\"POST\", endpoint)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    elif response.status_code == 202:\n",
    "        operation_url = response.headers.get(\"Location\")\n",
    "        if operation_url:\n",
    "            return wait_for_lro(operation_url)\n",
    "    \n",
    "    print(f\"Failed to get definition: {response.status_code}\")\n",
    "    print(response.text)\n",
    "    return {}\n",
    "\n",
    "\n",
    "def update_ontology_definition(workspace_id: str, ontology_id: str, definition_parts: list) -> dict:\n",
    "    \"\"\"\n",
    "    Update the ontology definition with new entity types, properties, and relationships.\n",
    "    \n",
    "    Args:\n",
    "        workspace_id: ID of the workspace\n",
    "        ontology_id: ID of the ontology to update\n",
    "        definition_parts: List of definition parts (each with path, payload, payloadType)\n",
    "    \n",
    "    Returns:\n",
    "        API response\n",
    "    \"\"\"\n",
    "    endpoint = f\"{FABRIC_API_VERSION}/workspaces/{workspace_id}/ontologies/{ontology_id}/updateDefinition\"\n",
    "    \n",
    "    data = {\n",
    "        \"definition\": {\n",
    "            \"parts\": definition_parts\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    response = api_request(\"POST\", endpoint, data=data)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        print(\"Definition updated successfully\")\n",
    "        return response.json()\n",
    "    elif response.status_code == 202:\n",
    "        # Long-running operation\n",
    "        operation_url = response.headers.get(\"Location\")\n",
    "        print(f\"Update is async. Polling for completion...\")\n",
    "        if operation_url:\n",
    "            return wait_for_lro(operation_url)\n",
    "    \n",
    "    print(f\"Failed to update definition: {response.status_code}\")\n",
    "    print(response.text)\n",
    "    raise Exception(f\"Update definition failed: {response.status_code} - {response.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c90924d",
   "metadata": {},
   "source": [
    "## Load Definition from File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4dbee51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latest_definition_file() -> str:\n",
    "    \"\"\"\n",
    "    Find the most recent ontology definition file.\n",
    "    \"\"\"\n",
    "    files = os.listdir(DEFINITIONS_DIR)\n",
    "    definition_files = [f for f in files if f.startswith(\"ontology_definition_\") and f.endswith(\".json\")]\n",
    "    \n",
    "    if not definition_files:\n",
    "        raise FileNotFoundError(f\"No definition files found in {DEFINITIONS_DIR}\")\n",
    "    \n",
    "    # Sort by timestamp in filename (YYYYMMDD_HHMMSS)\n",
    "    definition_files.sort(reverse=True)\n",
    "    return os.path.join(DEFINITIONS_DIR, definition_files[0])\n",
    "\n",
    "\n",
    "def load_definition(file_path: str) -> dict:\n",
    "    \"\"\"\n",
    "    Load ontology definition from JSON file.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "\n",
    "# Load the latest definition\n",
    "definition_file = get_latest_definition_file()\n",
    "print(f\"Loading definition from: {definition_file}\")\n",
    "\n",
    "full_definition = load_definition(definition_file)\n",
    "metadata = full_definition.get(\"metadata\", {})\n",
    "definition_parts = full_definition.get(\"definition\", {}).get(\"parts\", [])\n",
    "\n",
    "print(f\"\\nDefinition metadata:\")\n",
    "print(f\"  Generated: {metadata.get('generated_at', 'unknown')}\")\n",
    "print(f\"  Entity types: {metadata.get('entity_type_count', 0)}\")\n",
    "print(f\"  Relationship types: {metadata.get('relationship_type_count', 0)}\")\n",
    "print(f\"  Total parts: {len(definition_parts)}\")\n",
    "\n",
    "# Debug: Inspect actual part content\n",
    "print(f\"\\n--- Debug: Inspecting definition structure ---\")\n",
    "for part in definition_parts[:3]:\n",
    "    print(f\"\\nPath: {part.get('path')}\")\n",
    "    # Decode and show content\n",
    "    import base64\n",
    "    payload = part.get('payload', '')\n",
    "    try:\n",
    "        decoded = base64.b64decode(payload).decode('utf-8')\n",
    "        print(f\"Content preview: {decoded[:300]}...\")\n",
    "    except:\n",
    "        print(f\"Payload type: {part.get('payloadType')}\")\n",
    "\n",
    "# Check if any entity type has properties with 'valueType'\n",
    "entity_parts = [p for p in definition_parts if \"EntityTypes\" in p.get(\"path\", \"\") and \"definition.json\" in p.get(\"path\", \"\")]\n",
    "if entity_parts:\n",
    "    print(f\"\\n--- Checking entity type property format ---\")\n",
    "    sample = entity_parts[0]\n",
    "    decoded = json.loads(base64.b64decode(sample['payload']).decode('utf-8'))\n",
    "    print(f\"Sample entity: {decoded.get('name')}\")\n",
    "    if decoded.get('properties'):\n",
    "        first_prop = decoded['properties'][0]\n",
    "        print(f\"First property keys: {list(first_prop.keys())}\")\n",
    "        print(f\"First property: {first_prop}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c104c1c",
   "metadata": {},
   "source": [
    "## Create or Get Ontology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75e57b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if ontology already exists\n",
    "existing_ontology = find_ontology_by_name(workspace_id, ONTOLOGY_DISPLAY_NAME)\n",
    "\n",
    "if existing_ontology:\n",
    "    ontology_id = existing_ontology[\"id\"]\n",
    "    print(f\"Found existing ontology: {ontology_id}\")\n",
    "    print(f\"  Display Name: {existing_ontology.get('displayName')}\")\n",
    "    print(f\"  Description: {existing_ontology.get('description')}\")\n",
    "else:\n",
    "    print(f\"Creating new ontology: {ONTOLOGY_DISPLAY_NAME}\")\n",
    "    result = create_ontology(workspace_id, ONTOLOGY_DISPLAY_NAME, ONTOLOGY_DESCRIPTION)\n",
    "    \n",
    "    # For LRO, the result may not contain 'id' directly\n",
    "    # Look up the created ontology by name\n",
    "    if \"id\" in result:\n",
    "        ontology_id = result[\"id\"]\n",
    "    else:\n",
    "        # LRO completed - find the ontology by name\n",
    "        created_ontology = find_ontology_by_name(workspace_id, ONTOLOGY_DISPLAY_NAME)\n",
    "        if created_ontology:\n",
    "            ontology_id = created_ontology[\"id\"]\n",
    "        else:\n",
    "            raise Exception(\"Ontology creation completed but could not find the created ontology\")\n",
    "    \n",
    "    print(f\"Created ontology: {ontology_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe5f092",
   "metadata": {},
   "source": [
    "## Upload Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a953611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the definition to the ontology\n",
    "print(f\"Uploading definition to ontology {ontology_id}...\")\n",
    "print(f\"  Parts to upload: {len(definition_parts)}\")\n",
    "\n",
    "try:\n",
    "    result = update_ontology_definition(workspace_id, ontology_id, definition_parts)\n",
    "    print(\"\\nDefinition upload complete!\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nDefinition upload failed: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca0eb3a",
   "metadata": {},
   "source": [
    "## Verify Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3340dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the definition was uploaded correctly\n",
    "print(\"Verifying uploaded definition...\")\n",
    "\n",
    "# The getDefinition API is also async - the result confirms the operation status\n",
    "result = get_ontology_definition(workspace_id, ontology_id)\n",
    "\n",
    "status = result.get(\"status\", \"Unknown\")\n",
    "if status == \"Succeeded\":\n",
    "    print(f\"\\n✓ Ontology definition upload verified!\")\n",
    "    print(f\"  Status: {status}\")\n",
    "    print(f\"  Completed: {result.get('lastUpdatedTimeUtc', 'N/A')}\")\n",
    "    print(f\"\\nExpected content:\")\n",
    "    print(f\"  Entity types: {metadata.get('entity_type_count', 'N/A')}\")\n",
    "    print(f\"  Relationship types: {metadata.get('relationship_type_count', 'N/A')}\")\n",
    "    print(f\"  Total parts uploaded: {len(definition_parts)}\")\n",
    "else:\n",
    "    print(f\"\\n⚠ Unexpected status: {status}\")\n",
    "    print(f\"  Error: {result.get('error', 'None')}\")\n",
    "\n",
    "print(\"\\n→ Check Fabric portal to view the ontology entity types\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e3ab5d",
   "metadata": {},
   "source": [
    "## Save Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb08e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save ontology configuration for other notebooks\n",
    "os.makedirs(CONFIG_DIR, exist_ok=True)\n",
    "\n",
    "config = {\n",
    "    \"ontology_id\": ontology_id,\n",
    "    \"workspace_id\": workspace_id,\n",
    "    \"lakehouse_id\": lakehouse_id,\n",
    "    \"display_name\": ONTOLOGY_DISPLAY_NAME,\n",
    "    \"updated_at\": datetime.now().isoformat(),\n",
    "    \"definition_file\": definition_file,\n",
    "    \"entity_type_count\": metadata.get(\"entity_type_count\", 0),\n",
    "    \"relationship_type_count\": metadata.get(\"relationship_type_count\", 0)\n",
    "}\n",
    "\n",
    "config_path = os.path.join(CONFIG_DIR, \"ontology_config.json\")\n",
    "with open(config_path, 'w') as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "\n",
    "print(f\"Saved configuration to: {config_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9af2737",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975b5bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"Fabric Ontology API Client Complete\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nOntology ID: {ontology_id}\")\n",
    "print(f\"Workspace ID: {workspace_id}\")\n",
    "print(f\"Display Name: {ONTOLOGY_DISPLAY_NAME}\")\n",
    "print(f\"\\nEntity Types: {metadata.get('entity_type_count', 0)}\")\n",
    "print(f\"Relationship Types: {metadata.get('relationship_type_count', 0)}\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"Next Steps:\")\n",
    "print(\"=\"*60)\n",
    "print(\"1. Run F5.3 (Data Binding) to connect gold tables to entity types\")\n",
    "print(\"2. Go to Fabric portal → Ontology → View your ontology\")\n",
    "print(\"3. Once data is bound, query via Fabric Graph!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
