{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93496f8a",
   "metadata": {},
   "source": [
    "# 07 - Ontology Definition Generator\n",
    "\n",
    "**Epic:** F5 - Fabric Ontology Integration  \n",
    "**Feature:** F5.1 - Ontology Definition Generator  \n",
    "**Priority:** P0\n",
    "\n",
    "## Purpose\n",
    "\n",
    "Generate Fabric Ontology definition from silver layer schema tables. The output is a structured definition with base64-encoded JSON parts, ready for upload via the Fabric Ontology REST API.\n",
    "\n",
    "## Input\n",
    "\n",
    "- `silver_node_types` - Node type definitions from RDF classes\n",
    "- `silver_properties` - Property definitions (datatype and object)\n",
    "\n",
    "## Output\n",
    "\n",
    "- Ontology definition JSON saved to `Files/ontology_definitions/`\n",
    "- Structure ready for `POST /ontologies/{id}/updateDefinition`\n",
    "\n",
    "## Definition Structure\n",
    "\n",
    "```\n",
    "definition.json                                    → Root definition\n",
    "EntityTypes/{id}/definition.json                   → Entity type (name, properties, key)  \n",
    "EntityTypes/{id}/DataBindings/{bindingId}.json     → Data binding to lakehouse table\n",
    "RelationshipTypes/{id}/definition.json             → Relationships between entity types\n",
    ".platform                                          → Metadata\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286ef8ee",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf9e6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import base64\n",
    "import hashlib\n",
    "from datetime import datetime\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StringType"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb90f24d",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9224e23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ontology configuration\n",
    "ONTOLOGY_NAMESPACE = \"rdftranslation\"  # Custom namespace for generated types\n",
    "ONTOLOGY_VERSION = \"1.0\"\n",
    "\n",
    "# Output path (relative to lakehouse Files)\n",
    "OUTPUT_DIR = \"Files/ontology_definitions\"\n",
    "\n",
    "# Fabric Ontology naming constraints\n",
    "# - 1-26 characters\n",
    "# - Alphanumeric + hyphens/underscores\n",
    "# - Start and end with alphanumeric\n",
    "MAX_NAME_LENGTH = 26\n",
    "\n",
    "# Reserved words that cannot be used as names in Fabric Ontology\n",
    "RESERVED_WORDS = {\n",
    "    \"node\", \"edge\", \"graph\", \"vertex\", \"source\", \"target\",\n",
    "    \"id\", \"label\", \"type\", \"name\", \"properties\", \"entity\",\n",
    "    \"relationship\", \"property\", \"ontology\", \"binding\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e98adf",
   "metadata": {},
   "source": [
    "## Verify Required Tables Exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e7adfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that required tables exist before proceeding (using Spark catalog)\n",
    "required_tables = [\"silver_node_types\", \"silver_properties\"]\n",
    "missing_tables = []\n",
    "\n",
    "for table in required_tables:\n",
    "    try:\n",
    "        spark.table(table).limit(1)\n",
    "    except Exception:\n",
    "        missing_tables.append(table)\n",
    "\n",
    "if missing_tables:\n",
    "    print(\"ERROR: Required tables not found:\")\n",
    "    for t in missing_tables:\n",
    "        print(f\"  - {t}\")\n",
    "    print(\"\\nPlease run the following notebooks first:\")\n",
    "    print(\"  - 01_rdf_parser.ipynb (creates bronze_triples)\")\n",
    "    print(\"  - 02_schema_analyzer.ipynb (creates bronze_schema_analysis)\")\n",
    "    print(\"  - 03_class_mapper.ipynb (creates silver_node_types)\")\n",
    "    print(\"  - 04_property_mapper.ipynb (creates silver_properties)\")\n",
    "    raise RuntimeError(f\"Missing required tables: {missing_tables}\")\n",
    "else:\n",
    "    print(\"All required tables exist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1c3c76",
   "metadata": {},
   "source": [
    "## Helper Functions - Naming & Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe3d342",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanitize_name(name: str) -> str:\n",
    "    \"\"\"\n",
    "    Sanitize a name for Fabric Ontology compatibility.\n",
    "    \n",
    "    Constraints:\n",
    "    - 1-26 characters\n",
    "    - Alphanumeric + hyphens/underscores\n",
    "    - Start and end with alphanumeric\n",
    "    \"\"\"\n",
    "    if name is None:\n",
    "        return \"Unknown\"\n",
    "    \n",
    "    # Remove namespace prefixes if present (take local name)\n",
    "    if \":\" in name and not name.startswith(\"http\"):\n",
    "        name = name.split(\":\")[-1]\n",
    "    \n",
    "    # For URIs, extract local name\n",
    "    if \"/\" in name:\n",
    "        name = name.rsplit(\"/\", 1)[-1]\n",
    "    if \"#\" in name:\n",
    "        name = name.rsplit(\"#\", 1)[-1]\n",
    "    \n",
    "    # Replace spaces and hyphens with underscores\n",
    "    name = re.sub(r\"[\\s-]+\", \"_\", name)\n",
    "    \n",
    "    # Remove special characters (keep only alphanumeric and underscore)\n",
    "    name = re.sub(r\"[^a-zA-Z0-9_]\", \"\", name)\n",
    "    \n",
    "    # Ensure starts with alphanumeric\n",
    "    while name and not name[0].isalnum():\n",
    "        name = name[1:]\n",
    "    if not name:\n",
    "        name = \"Unknown\"\n",
    "    if not name[0].isalnum():\n",
    "        name = \"E\" + name\n",
    "    \n",
    "    # Ensure ends with alphanumeric\n",
    "    while name and not name[-1].isalnum():\n",
    "        name = name[:-1]\n",
    "    \n",
    "    # Handle reserved words\n",
    "    if name.lower() in RESERVED_WORDS:\n",
    "        name = name + \"Type\"\n",
    "    \n",
    "    # Truncate to max length while keeping alphanumeric end\n",
    "    if len(name) > MAX_NAME_LENGTH:\n",
    "        name = name[:MAX_NAME_LENGTH]\n",
    "        while name and not name[-1].isalnum():\n",
    "            name = name[:-1]\n",
    "    \n",
    "    return name if name else \"Unknown\"\n",
    "\n",
    "\n",
    "def generate_id(seed: str) -> str:\n",
    "    \"\"\"\n",
    "    Generate a deterministic numeric ID from a seed string.\n",
    "    Used for entity type IDs, property IDs, etc.\n",
    "    \"\"\"\n",
    "    # Use MD5 hash and take first 13 digits for a numeric ID\n",
    "    hash_hex = hashlib.md5(seed.encode('utf-8')).hexdigest()\n",
    "    # Convert hex to decimal and take first 13 digits\n",
    "    numeric_id = str(int(hash_hex[:13], 16))[:13]\n",
    "    return numeric_id\n",
    "\n",
    "\n",
    "def validate_name(name: str) -> tuple[bool, str]:\n",
    "    \"\"\"\n",
    "    Validate a name against Fabric Ontology rules.\n",
    "    Returns (is_valid, error_message)\n",
    "    \"\"\"\n",
    "    if not name:\n",
    "        return False, \"Name cannot be empty\"\n",
    "    \n",
    "    if len(name) < 1 or len(name) > 26:\n",
    "        return False, f\"Name must be 1-26 chars, got {len(name)}\"\n",
    "    \n",
    "    if not name[0].isalnum():\n",
    "        return False, f\"Name must start with alphanumeric, got '{name[0]}'\"\n",
    "    \n",
    "    if not name[-1].isalnum():\n",
    "        return False, f\"Name must end with alphanumeric, got '{name[-1]}'\"\n",
    "    \n",
    "    if not re.match(r'^[a-zA-Z0-9][a-zA-Z0-9_-]*[a-zA-Z0-9]$|^[a-zA-Z0-9]$', name):\n",
    "        return False, f\"Name contains invalid characters: '{name}'\"\n",
    "    \n",
    "    return True, \"\"\n",
    "\n",
    "\n",
    "# Test sanitize_name\n",
    "test_cases = [\n",
    "    (\"Person\", \"Person\"),\n",
    "    (\"my_class\", \"my_class\"),\n",
    "    (\"This Is A Very Long Name That Should Be Truncated\", None),  # Should be <= 26\n",
    "    (\"123numeric\", \"E123numeric\"),  # Should add prefix\n",
    "    (\"http://example.org/Person\", \"Person\"),  # URI extraction\n",
    "    (\"ex:Person\", \"Person\"),  # Prefix removal\n",
    "    (\"id\", \"idType\"),  # Reserved word\n",
    "]\n",
    "\n",
    "print(\"Testing sanitize_name():\")\n",
    "for input_name, expected in test_cases:\n",
    "    result = sanitize_name(input_name)\n",
    "    is_valid, error = validate_name(result)\n",
    "    status = \"✓\" if is_valid else f\"✗ {error}\"\n",
    "    print(f\"  '{input_name}' → '{result}' [{status}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2619810",
   "metadata": {},
   "source": [
    "## Helper Functions - Datatype Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d257ae3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_to_ontology_datatype(rdf_datatype: str) -> str:\n",
    "    \"\"\"\n",
    "    Map RDF/XSD datatypes to Fabric Ontology types.\n",
    "    \n",
    "    Fabric Ontology supports: String, Int32, Int64, Double, Boolean, DateTime\n",
    "    \"\"\"\n",
    "    if rdf_datatype is None:\n",
    "        return \"String\"  # Default to String\n",
    "    \n",
    "    datatype_lower = rdf_datatype.lower()\n",
    "    \n",
    "    # String types\n",
    "    if any(t in datatype_lower for t in [\"string\", \"langstring\", \"anyuri\", \"token\", \"normalizedstring\", \"literal\"]):\n",
    "        return \"String\"\n",
    "    \n",
    "    # 32-bit Integer types\n",
    "    if any(t in datatype_lower for t in [\"int\", \"short\", \"byte\", \"unsignedshort\", \"unsignedbyte\"]):\n",
    "        return \"Int32\"\n",
    "    \n",
    "    # 64-bit Integer types (larger ranges)\n",
    "    if any(t in datatype_lower for t in [\"integer\", \"long\", \"unsignedint\", \"unsignedlong\", \"nonpositiveinteger\", \"nonnegativeinteger\", \"positiveinteger\", \"negativeinteger\"]):\n",
    "        return \"Int64\"\n",
    "    \n",
    "    # Double/Float types\n",
    "    if any(t in datatype_lower for t in [\"double\", \"float\", \"decimal\"]):\n",
    "        return \"Double\"\n",
    "    \n",
    "    # Boolean\n",
    "    if \"boolean\" in datatype_lower:\n",
    "        return \"Boolean\"\n",
    "    \n",
    "    # DateTime types\n",
    "    if any(t in datatype_lower for t in [\"datetime\", \"date\", \"time\", \"gyear\", \"gmonth\", \"gday\"]):\n",
    "        return \"DateTime\"\n",
    "    \n",
    "    # Default to String for unknown types\n",
    "    return \"String\"\n",
    "\n",
    "\n",
    "# Test datatype mapping\n",
    "test_types = [\n",
    "    (\"xsd:string\", \"String\"),\n",
    "    (\"xsd:integer\", \"Int64\"),\n",
    "    (\"xsd:int\", \"Int32\"),\n",
    "    (\"xsd:double\", \"Double\"),\n",
    "    (\"xsd:boolean\", \"Boolean\"),\n",
    "    (\"xsd:dateTime\", \"DateTime\"),\n",
    "    (\"http://www.w3.org/2001/XMLSchema#string\", \"String\"),\n",
    "    (None, \"String\"),\n",
    "]\n",
    "\n",
    "print(\"Testing map_to_ontology_datatype():\")\n",
    "for rdf_type, expected in test_types:\n",
    "    result = map_to_ontology_datatype(rdf_type)\n",
    "    status = \"✓\" if result == expected else f\"✗ expected {expected}\"\n",
    "    print(f\"  {rdf_type} → {result} [{status}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4655f8",
   "metadata": {},
   "source": [
    "## Helper Functions - Base64 Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e11febb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_payload(data: dict) -> str:\n",
    "    \"\"\"\n",
    "    Encode a dictionary as base64 JSON string for Fabric API.\n",
    "    \"\"\"\n",
    "    json_str = json.dumps(data, indent=2)\n",
    "    return base64.b64encode(json_str.encode('utf-8')).decode('utf-8')\n",
    "\n",
    "\n",
    "def decode_payload(encoded: str) -> dict:\n",
    "    \"\"\"\n",
    "    Decode a base64 JSON string back to a dictionary.\n",
    "    \"\"\"\n",
    "    json_str = base64.b64decode(encoded.encode('utf-8')).decode('utf-8')\n",
    "    return json.loads(json_str)\n",
    "\n",
    "\n",
    "# Test encoding/decoding\n",
    "test_data = {\"name\": \"Test\", \"value\": 123}\n",
    "encoded = encode_payload(test_data)\n",
    "decoded = decode_payload(encoded)\n",
    "print(f\"Encoding test: {test_data} → '{encoded[:30]}...'\")\n",
    "print(f\"Decode matches original: {decoded == test_data}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4f514c",
   "metadata": {},
   "source": [
    "## Load Schema Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d4788b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load node types\n",
    "df_node_types = spark.table(\"silver_node_types\")\n",
    "print(f\"Loaded {df_node_types.count()} node types\")\n",
    "df_node_types.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f403875e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load properties\n",
    "df_properties = spark.table(\"silver_properties\")\n",
    "print(f\"Loaded {df_properties.count()} properties\")\n",
    "df_properties.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4f66ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate datatype and object properties\n",
    "df_datatype_props = df_properties.filter(F.col(\"property_type\") == \"datatype\")\n",
    "df_object_props = df_properties.filter(F.col(\"property_type\") == \"object\")\n",
    "\n",
    "print(f\"Datatype properties (become entity properties): {df_datatype_props.count()}\")\n",
    "print(f\"Object properties (become relationship types): {df_object_props.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20594c5a",
   "metadata": {},
   "source": [
    "## Build Entity Type Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1424d21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect node types to build entity types\n",
    "node_types = df_node_types.select(\n",
    "    \"node_type\", \"class_uri\", \"display_name\", \"description\"\n",
    ").collect()\n",
    "\n",
    "# Build lookup for node_type to entity_id mapping (for relationships)\n",
    "node_type_to_entity_id = {}\n",
    "for row in node_types:\n",
    "    entity_id = generate_id(f\"entity_{row['class_uri']}\")\n",
    "    node_type_to_entity_id[row['node_type']] = entity_id\n",
    "    node_type_to_entity_id[row['class_uri']] = entity_id  # Also index by URI\n",
    "\n",
    "# Collect datatype properties with their domains\n",
    "datatype_props = df_datatype_props.select(\n",
    "    \"property_name\", \"property_uri\", \"data_type\", \"source_types\"\n",
    ").collect()\n",
    "\n",
    "print(f\"Building entity type definitions for {len(node_types)} types\")\n",
    "print(f\"Using {len(datatype_props)} datatype properties\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f1a136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build entity type definitions\n",
    "entity_types = []\n",
    "\n",
    "for node_row in node_types:\n",
    "    node_type = node_row[\"node_type\"]\n",
    "    class_uri = node_row[\"class_uri\"]\n",
    "    \n",
    "    # Generate entity ID\n",
    "    entity_id = node_type_to_entity_id[node_type]\n",
    "    \n",
    "    # Sanitize entity name\n",
    "    entity_name = sanitize_name(node_type)\n",
    "    \n",
    "    # Build properties list\n",
    "    # Start with required 'uri' property for the original RDF IRI\n",
    "    uri_prop_id = generate_id(f\"prop_{class_uri}_uri\")\n",
    "    properties = [\n",
    "        {\n",
    "            \"id\": uri_prop_id,\n",
    "            \"name\": \"uri\",\n",
    "            \"dataType\": \"String\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Find datatype properties for this entity type\n",
    "    seen_prop_names = {\"uri\"}\n",
    "    \n",
    "    for prop_row in datatype_props:\n",
    "        source_types = prop_row[\"source_types\"] or []\n",
    "        \n",
    "        # Check if this property applies to this entity\n",
    "        if node_type in source_types or class_uri in source_types:\n",
    "            prop_name = sanitize_name(prop_row[\"property_name\"])\n",
    "            prop_uri = prop_row[\"property_uri\"]\n",
    "            \n",
    "            # Avoid duplicate property names\n",
    "            if prop_name not in seen_prop_names:\n",
    "                prop_id = generate_id(f\"prop_{class_uri}_{prop_uri}\")\n",
    "                prop_type = map_to_ontology_datatype(prop_row[\"data_type\"])\n",
    "                \n",
    "                properties.append({\n",
    "                    \"id\": prop_id,\n",
    "                    \"name\": prop_name,\n",
    "                    \"dataType\": prop_type\n",
    "                })\n",
    "                seen_prop_names.add(prop_name)\n",
    "    \n",
    "    # Build entity type definition\n",
    "    entity_definition = {\n",
    "        \"id\": entity_id,\n",
    "        \"namespace\": ONTOLOGY_NAMESPACE,\n",
    "        \"name\": entity_name,\n",
    "        \"entityIdParts\": [uri_prop_id],  # Use URI as entity key\n",
    "        \"displayNamePropertyId\": uri_prop_id,  # Use URI as display name\n",
    "        \"properties\": properties\n",
    "    }\n",
    "    \n",
    "    entity_types.append({\n",
    "        \"id\": entity_id,\n",
    "        \"name\": entity_name,\n",
    "        \"class_uri\": class_uri,\n",
    "        \"definition\": entity_definition\n",
    "    })\n",
    "\n",
    "print(f\"\\nBuilt {len(entity_types)} entity type definitions:\")\n",
    "for et in entity_types:\n",
    "    prop_count = len(et['definition']['properties'])\n",
    "    print(f\"  {et['name']}: {prop_count} properties\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3bca1a",
   "metadata": {},
   "source": [
    "## Build Relationship Type Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4c0c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect object properties (these become relationship types)\n",
    "object_props = df_object_props.select(\n",
    "    \"property_name\", \"property_uri\", \"source_types\", \"target_types\"\n",
    ").collect()\n",
    "\n",
    "print(f\"Building relationship type definitions from {len(object_props)} object properties\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad15917b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build relationship type definitions\n",
    "relationship_types = []\n",
    "\n",
    "# Track entity names for lookup\n",
    "entity_name_to_id = {et['name']: et['id'] for et in entity_types}\n",
    "node_type_to_entity_name = {et['class_uri']: et['name'] for et in entity_types}\n",
    "for et in entity_types:\n",
    "    # Also map by sanitized node_type name\n",
    "    node_type_to_entity_name[et['name']] = et['name']\n",
    "\n",
    "for prop_row in object_props:\n",
    "    prop_name = prop_row[\"property_name\"]\n",
    "    prop_uri = prop_row[\"property_uri\"]\n",
    "    source_types = prop_row[\"source_types\"] or []\n",
    "    target_types = prop_row[\"target_types\"] or []\n",
    "    \n",
    "    # Sanitize relationship name\n",
    "    rel_name = sanitize_name(prop_name)\n",
    "    \n",
    "    # Generate relationship ID\n",
    "    rel_id = generate_id(f\"rel_{prop_uri}\")\n",
    "    \n",
    "    # Find source entity types\n",
    "    sources = []\n",
    "    for st in source_types:\n",
    "        if st in node_type_to_entity_id:\n",
    "            sources.append(node_type_to_entity_id[st])\n",
    "    \n",
    "    # Find target entity types\n",
    "    targets = []\n",
    "    for tt in target_types:\n",
    "        if tt in node_type_to_entity_id:\n",
    "            targets.append(node_type_to_entity_id[tt])\n",
    "    \n",
    "    # Skip if we can't resolve source or target\n",
    "    if not sources or not targets:\n",
    "        print(f\"  Warning: Skipping '{rel_name}' - unresolved source/target\")\n",
    "        print(f\"    Sources: {source_types} → {sources}\")\n",
    "        print(f\"    Targets: {target_types} → {targets}\")\n",
    "        continue\n",
    "    \n",
    "    # Build relationship definition\n",
    "    # Note: Fabric allows multiple source/target entity types\n",
    "    relationship_definition = {\n",
    "        \"id\": rel_id,\n",
    "        \"namespace\": ONTOLOGY_NAMESPACE,\n",
    "        \"name\": rel_name,\n",
    "        \"fromEntityTypeIds\": list(set(sources)),  # Deduplicate\n",
    "        \"toEntityTypeIds\": list(set(targets)),    # Deduplicate\n",
    "        \"properties\": []  # Relationships can have properties too\n",
    "    }\n",
    "    \n",
    "    relationship_types.append({\n",
    "        \"id\": rel_id,\n",
    "        \"name\": rel_name,\n",
    "        \"property_uri\": prop_uri,\n",
    "        \"definition\": relationship_definition\n",
    "    })\n",
    "\n",
    "print(f\"\\nBuilt {len(relationship_types)} relationship type definitions:\")\n",
    "for rt in relationship_types[:10]:  # Show first 10\n",
    "    from_count = len(rt['definition']['fromEntityTypeIds'])\n",
    "    to_count = len(rt['definition']['toEntityTypeIds'])\n",
    "    print(f\"  {rt['name']}: {from_count} source(s) → {to_count} target(s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa473a90",
   "metadata": {},
   "source": [
    "## Assemble Definition Parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d05baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assemble the full definition structure with base64-encoded parts\n",
    "definition_parts = []\n",
    "\n",
    "# Add .platform metadata\n",
    "platform_metadata = {\n",
    "    \"$schema\": \"https://developer.microsoft.com/json-schemas/fabric/gitIntegration/platformProperties/2.0.0/schema.json\",\n",
    "    \"metadata\": {\n",
    "        \"type\": \"Ontology\",\n",
    "        \"displayName\": \"RDF Translated Ontology\"\n",
    "    },\n",
    "    \"config\": {\n",
    "        \"version\": ONTOLOGY_VERSION\n",
    "    }\n",
    "}\n",
    "definition_parts.append({\n",
    "    \"path\": \".platform\",\n",
    "    \"payload\": encode_payload(platform_metadata),\n",
    "    \"payloadType\": \"InlineBase64\"\n",
    "})\n",
    "\n",
    "# Add root definition.json\n",
    "root_definition = {\n",
    "    \"version\": ONTOLOGY_VERSION,\n",
    "    \"namespace\": ONTOLOGY_NAMESPACE,\n",
    "    \"entityTypeIds\": [et['id'] for et in entity_types],\n",
    "    \"relationshipTypeIds\": [rt['id'] for rt in relationship_types]\n",
    "}\n",
    "definition_parts.append({\n",
    "    \"path\": \"definition.json\",\n",
    "    \"payload\": encode_payload(root_definition),\n",
    "    \"payloadType\": \"InlineBase64\"\n",
    "})\n",
    "\n",
    "print(f\"Added root definition with {len(entity_types)} entity types, {len(relationship_types)} relationship types\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46bc9766",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add entity type definitions\n",
    "for et in entity_types:\n",
    "    path = f\"EntityTypes/{et['id']}/definition.json\"\n",
    "    definition_parts.append({\n",
    "        \"path\": path,\n",
    "        \"payload\": encode_payload(et['definition']),\n",
    "        \"payloadType\": \"InlineBase64\"\n",
    "    })\n",
    "\n",
    "print(f\"Added {len(entity_types)} entity type definition parts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec92026f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add relationship type definitions\n",
    "for rt in relationship_types:\n",
    "    path = f\"RelationshipTypes/{rt['id']}/definition.json\"\n",
    "    definition_parts.append({\n",
    "        \"path\": path,\n",
    "        \"payload\": encode_payload(rt['definition']),\n",
    "        \"payloadType\": \"InlineBase64\"\n",
    "    })\n",
    "\n",
    "print(f\"Added {len(relationship_types)} relationship type definition parts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093da951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of all parts\n",
    "print(f\"\\nTotal definition parts: {len(definition_parts)}\")\n",
    "print(\"\\nPart paths:\")\n",
    "for part in definition_parts[:15]:  # Show first 15\n",
    "    print(f\"  {part['path']}\")\n",
    "if len(definition_parts) > 15:\n",
    "    print(f\"  ... and {len(definition_parts) - 15} more\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7227d2d8",
   "metadata": {},
   "source": [
    "## Validate Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbe5726",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_ontology_definition(parts: list[dict]) -> tuple[bool, list[str]]:\n",
    "    \"\"\"\n",
    "    Validate the ontology definition structure.\n",
    "    \n",
    "    Returns (is_valid, list of errors)\n",
    "    \"\"\"\n",
    "    errors = []\n",
    "    \n",
    "    # Check required files exist\n",
    "    paths = {p['path'] for p in parts}\n",
    "    if '.platform' not in paths:\n",
    "        errors.append(\"Missing .platform metadata\")\n",
    "    if 'definition.json' not in paths:\n",
    "        errors.append(\"Missing definition.json\")\n",
    "    \n",
    "    # Validate each part\n",
    "    for part in parts:\n",
    "        try:\n",
    "            payload = decode_payload(part['payload'])\n",
    "            \n",
    "            # Validate entity type definitions\n",
    "            if part['path'].startswith('EntityTypes/') and part['path'].endswith('/definition.json'):\n",
    "                required_fields = ['id', 'namespace', 'name', 'properties']\n",
    "                for field in required_fields:\n",
    "                    if field not in payload:\n",
    "                        errors.append(f\"Entity {part['path']}: missing {field}\")\n",
    "                \n",
    "                # Validate name\n",
    "                if 'name' in payload:\n",
    "                    is_valid, error = validate_name(payload['name'])\n",
    "                    if not is_valid:\n",
    "                        errors.append(f\"Entity {part['path']}: {error}\")\n",
    "                \n",
    "                # Validate properties\n",
    "                for i, prop in enumerate(payload.get('properties', [])):\n",
    "                    for pf in ['id', 'name', 'dataType']:\n",
    "                        if pf not in prop:\n",
    "                            errors.append(f\"Entity {part['path']}, property {i}: missing {pf}\")\n",
    "            \n",
    "            # Validate relationship type definitions\n",
    "            if part['path'].startswith('RelationshipTypes/') and part['path'].endswith('/definition.json'):\n",
    "                required_fields = ['id', 'namespace', 'name', 'fromEntityTypeIds', 'toEntityTypeIds']\n",
    "                for field in required_fields:\n",
    "                    if field not in payload:\n",
    "                        errors.append(f\"Relationship {part['path']}: missing {field}\")\n",
    "                \n",
    "                # Validate name\n",
    "                if 'name' in payload:\n",
    "                    is_valid, error = validate_name(payload['name'])\n",
    "                    if not is_valid:\n",
    "                        errors.append(f\"Relationship {part['path']}: {error}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            errors.append(f\"Part {part['path']}: decode error - {e}\")\n",
    "    \n",
    "    return len(errors) == 0, errors\n",
    "\n",
    "\n",
    "# Validate\n",
    "is_valid, validation_errors = validate_ontology_definition(definition_parts)\n",
    "\n",
    "if is_valid:\n",
    "    print(\"✓ Ontology definition is valid\")\n",
    "else:\n",
    "    print(f\"✗ Ontology definition has {len(validation_errors)} errors:\")\n",
    "    for error in validation_errors[:20]:  # Show first 20\n",
    "        print(f\"  - {error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5aa18b",
   "metadata": {},
   "source": [
    "## Save Definition to Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c3fec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory if it doesn't exist\n",
    "import os\n",
    "\n",
    "# In Fabric, Files/ is a mounted path\n",
    "output_dir = \"/lakehouse/default/Files/ontology_definitions\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "print(f\"Output directory: {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aeae177",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the full definition (for API upload)\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "definition_filename = f\"ontology_definition_{timestamp}.json\"\n",
    "\n",
    "full_definition = {\n",
    "    \"metadata\": {\n",
    "        \"generated_at\": datetime.now().isoformat(),\n",
    "        \"generator\": \"07_ontology_definition_generator\",\n",
    "        \"version\": ONTOLOGY_VERSION,\n",
    "        \"entity_type_count\": len(entity_types),\n",
    "        \"relationship_type_count\": len(relationship_types)\n",
    "    },\n",
    "    \"definition\": {\n",
    "        \"parts\": definition_parts\n",
    "    }\n",
    "}\n",
    "\n",
    "definition_path = os.path.join(output_dir, definition_filename)\n",
    "with open(definition_path, 'w') as f:\n",
    "    json.dump(full_definition, f, indent=2)\n",
    "\n",
    "print(f\"Saved: {definition_filename}\")\n",
    "print(f\"  Entity types: {len(entity_types)}\")\n",
    "print(f\"  Relationship types: {len(relationship_types)}\")\n",
    "print(f\"  Definition parts: {len(definition_parts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac25125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also save human-readable versions for debugging\n",
    "\n",
    "# Entity types summary\n",
    "entity_summary = []\n",
    "for et in entity_types:\n",
    "    entity_summary.append({\n",
    "        \"id\": et['id'],\n",
    "        \"name\": et['name'],\n",
    "        \"class_uri\": et['class_uri'],\n",
    "        \"property_count\": len(et['definition']['properties']),\n",
    "        \"properties\": [p['name'] for p in et['definition']['properties']]\n",
    "    })\n",
    "\n",
    "entity_summary_path = os.path.join(output_dir, f\"entity_types_{timestamp}.json\")\n",
    "with open(entity_summary_path, 'w') as f:\n",
    "    json.dump(entity_summary, f, indent=2)\n",
    "\n",
    "# Relationship types summary\n",
    "rel_summary = []\n",
    "for rt in relationship_types:\n",
    "    rel_summary.append({\n",
    "        \"id\": rt['id'],\n",
    "        \"name\": rt['name'],\n",
    "        \"property_uri\": rt['property_uri'],\n",
    "        \"from_entity_ids\": rt['definition']['fromEntityTypeIds'],\n",
    "        \"to_entity_ids\": rt['definition']['toEntityTypeIds']\n",
    "    })\n",
    "\n",
    "rel_summary_path = os.path.join(output_dir, f\"relationship_types_{timestamp}.json\")\n",
    "with open(rel_summary_path, 'w') as f:\n",
    "    json.dump(rel_summary, f, indent=2)\n",
    "\n",
    "print(f\"\\nSaved human-readable summaries:\")\n",
    "print(f\"  entity_types_{timestamp}.json\")\n",
    "print(f\"  relationship_types_{timestamp}.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be297fe6",
   "metadata": {},
   "source": [
    "## Summary & Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcfed9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"Ontology Definition Generation Complete\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nGenerated:\")\n",
    "print(f\"  - {len(entity_types)} entity types\")\n",
    "print(f\"  - {len(relationship_types)} relationship types\")\n",
    "print(f\"  - {len(definition_parts)} definition parts\")\n",
    "\n",
    "print(f\"\\nOutput files in {output_dir}:\")\n",
    "print(f\"  - {definition_filename} (API-ready definition)\")\n",
    "print(f\"  - entity_types_{timestamp}.json (human-readable)\")\n",
    "print(f\"  - relationship_types_{timestamp}.json (human-readable)\")\n",
    "\n",
    "print(f\"\\nValidation: {'✓ PASSED' if is_valid else '✗ FAILED'}\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"Next Steps:\")\n",
    "print(\"=\"*60)\n",
    "print(\"1. Run F5.2 (Fabric Ontology REST API Client) to upload definition\")\n",
    "print(\"2. Run F5.3 (Lakehouse Data Binding) to bind gold tables\")\n",
    "print(\"3. Query the materialized graph via Fabric Graph!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d11ac8",
   "metadata": {},
   "source": [
    "## Display Generated Definition (Sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921a3be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show first entity type definition (decoded) as example\n",
    "if entity_types:\n",
    "    print(\"Sample Entity Type Definition:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(json.dumps(entity_types[0]['definition'], indent=2))\n",
    "\n",
    "print()\n",
    "\n",
    "# Show first relationship type definition (decoded) as example\n",
    "if relationship_types:\n",
    "    print(\"Sample Relationship Type Definition:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(json.dumps(relationship_types[0]['definition'], indent=2))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
