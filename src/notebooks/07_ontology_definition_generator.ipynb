{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93496f8a",
   "metadata": {},
   "source": [
    "# 07 - Ontology Definition Generator\n",
    "\n",
    "**Epic:** F5 - Fabric Ontology Integration  \n",
    "**Feature:** F5.1 - Ontology Definition Generator  \n",
    "**Priority:** P0\n",
    "\n",
    "## Purpose\n",
    "\n",
    "Generate Fabric Ontology definition from silver layer schema tables. The output is a structured definition with base64-encoded JSON parts, ready for upload via the Fabric Ontology REST API.\n",
    "\n",
    "## Input\n",
    "\n",
    "- `silver_node_types` - Node type definitions from RDF classes\n",
    "- `silver_properties` - Property definitions (datatype and object)\n",
    "\n",
    "## Output\n",
    "\n",
    "- Ontology definition JSON saved to `Files/ontology_definitions/`\n",
    "- Structure ready for `POST /ontologies/{id}/updateDefinition`\n",
    "\n",
    "## Definition Structure\n",
    "\n",
    "```\n",
    "definition.json                                    → Root definition\n",
    "EntityTypes/{id}/definition.json                   → Entity type (name, properties, key)  \n",
    "EntityTypes/{id}/DataBindings/{bindingId}.json     → Data binding to lakehouse table\n",
    "RelationshipTypes/{id}/definition.json             → Relationships between entity types\n",
    ".platform                                          → Metadata\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286ef8ee",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf9e6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import base64\n",
    "import hashlib\n",
    "from datetime import datetime\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StringType"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb90f24d",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9224e23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ontology configuration\n",
    "ONTOLOGY_NAMESPACE = \"rdftranslation\"  # Custom namespace for generated types\n",
    "ONTOLOGY_VERSION = \"1.0\"\n",
    "\n",
    "# Output path (relative to lakehouse Files)\n",
    "OUTPUT_DIR = \"Files/ontology_definitions\"\n",
    "\n",
    "# Fabric Ontology naming constraints\n",
    "# - 1-26 characters\n",
    "# - Alphanumeric + hyphens/underscores\n",
    "# - Start and end with alphanumeric\n",
    "MAX_NAME_LENGTH = 26\n",
    "\n",
    "# Reserved words that cannot be used as names in Fabric Ontology\n",
    "RESERVED_WORDS = {\n",
    "    \"node\", \"edge\", \"graph\", \"vertex\", \"source\", \"target\",\n",
    "    \"id\", \"label\", \"type\", \"name\", \"properties\", \"entity\",\n",
    "    \"relationship\", \"property\", \"ontology\", \"binding\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e98adf",
   "metadata": {},
   "source": [
    "## Verify Required Tables Exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e7adfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that required tables exist before proceeding (using Spark catalog)\n",
    "required_tables = [\"silver_node_types\", \"silver_properties\"]\n",
    "missing_tables = []\n",
    "\n",
    "for table in required_tables:\n",
    "    try:\n",
    "        spark.table(table).limit(1)\n",
    "    except Exception:\n",
    "        missing_tables.append(table)\n",
    "\n",
    "if missing_tables:\n",
    "    print(\"ERROR: Required tables not found:\")\n",
    "    for t in missing_tables:\n",
    "        print(f\"  - {t}\")\n",
    "    print(\"\\nPlease run the following notebooks first:\")\n",
    "    print(\"  - 01_rdf_parser.ipynb (creates bronze_triples)\")\n",
    "    print(\"  - 02_schema_analyzer.ipynb (creates bronze_schema_analysis)\")\n",
    "    print(\"  - 03_class_mapper.ipynb (creates silver_node_types)\")\n",
    "    print(\"  - 04_property_mapper.ipynb (creates silver_properties)\")\n",
    "    raise RuntimeError(f\"Missing required tables: {missing_tables}\")\n",
    "else:\n",
    "    print(\"All required tables exist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1c3c76",
   "metadata": {},
   "source": [
    "## Helper Functions - Naming & Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe3d342",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanitize_name(name: str) -> str:\n",
    "    \"\"\"\n",
    "    Sanitize a name for Fabric Ontology compatibility.\n",
    "    \n",
    "    Constraints:\n",
    "    - 1-26 characters\n",
    "    - Alphanumeric + hyphens/underscores\n",
    "    - Start and end with alphanumeric\n",
    "    \"\"\"\n",
    "    if name is None:\n",
    "        return \"Unknown\"\n",
    "    \n",
    "    # Remove namespace prefixes if present (take local name)\n",
    "    if \":\" in name and not name.startswith(\"http\"):\n",
    "        name = name.split(\":\")[-1]\n",
    "    \n",
    "    # For URIs, extract local name\n",
    "    if \"/\" in name:\n",
    "        name = name.rsplit(\"/\", 1)[-1]\n",
    "    if \"#\" in name:\n",
    "        name = name.rsplit(\"#\", 1)[-1]\n",
    "    \n",
    "    # Replace spaces and hyphens with underscores\n",
    "    name = re.sub(r\"[\\s-]+\", \"_\", name)\n",
    "    \n",
    "    # Remove special characters (keep only alphanumeric and underscore)\n",
    "    name = re.sub(r\"[^a-zA-Z0-9_]\", \"\", name)\n",
    "    \n",
    "    # Ensure starts with alphanumeric\n",
    "    while name and not name[0].isalnum():\n",
    "        name = name[1:]\n",
    "    if not name:\n",
    "        name = \"Unknown\"\n",
    "    if not name[0].isalnum():\n",
    "        name = \"E\" + name\n",
    "    \n",
    "    # Ensure ends with alphanumeric\n",
    "    while name and not name[-1].isalnum():\n",
    "        name = name[:-1]\n",
    "    \n",
    "    # Handle reserved words\n",
    "    if name.lower() in RESERVED_WORDS:\n",
    "        name = name + \"Type\"\n",
    "    \n",
    "    # Truncate to max length while keeping alphanumeric end\n",
    "    if len(name) > MAX_NAME_LENGTH:\n",
    "        name = name[:MAX_NAME_LENGTH]\n",
    "        while name and not name[-1].isalnum():\n",
    "            name = name[:-1]\n",
    "    \n",
    "    return name if name else \"Unknown\"\n",
    "\n",
    "\n",
    "def generate_id(seed: str) -> str:\n",
    "    \"\"\"\n",
    "    Generate a deterministic numeric ID from a seed string.\n",
    "    Used for entity type IDs, property IDs, etc.\n",
    "    \"\"\"\n",
    "    # Use MD5 hash and take first 13 digits for a numeric ID\n",
    "    hash_hex = hashlib.md5(seed.encode('utf-8')).hexdigest()\n",
    "    # Convert hex to decimal and take first 13 digits\n",
    "    numeric_id = str(int(hash_hex[:13], 16))[:13]\n",
    "    return numeric_id\n",
    "\n",
    "\n",
    "def validate_name(name: str) -> tuple[bool, str]:\n",
    "    \"\"\"\n",
    "    Validate a name against Fabric Ontology rules.\n",
    "    Returns (is_valid, error_message)\n",
    "    \"\"\"\n",
    "    if not name:\n",
    "        return False, \"Name cannot be empty\"\n",
    "    \n",
    "    if len(name) < 1 or len(name) > 26:\n",
    "        return False, f\"Name must be 1-26 chars, got {len(name)}\"\n",
    "    \n",
    "    if not name[0].isalnum():\n",
    "        return False, f\"Name must start with alphanumeric, got '{name[0]}'\"\n",
    "    \n",
    "    if not name[-1].isalnum():\n",
    "        return False, f\"Name must end with alphanumeric, got '{name[-1]}'\"\n",
    "    \n",
    "    if not re.match(r'^[a-zA-Z0-9][a-zA-Z0-9_-]*[a-zA-Z0-9]$|^[a-zA-Z0-9]$', name):\n",
    "        return False, f\"Name contains invalid characters: '{name}'\"\n",
    "    \n",
    "    return True, \"\"\n",
    "\n",
    "\n",
    "# Test sanitize_name\n",
    "test_cases = [\n",
    "    (\"Person\", \"Person\"),\n",
    "    (\"my_class\", \"my_class\"),\n",
    "    (\"This Is A Very Long Name That Should Be Truncated\", None),  # Should be <= 26\n",
    "    (\"123numeric\", \"E123numeric\"),  # Should add prefix\n",
    "    (\"http://example.org/Person\", \"Person\"),  # URI extraction\n",
    "    (\"ex:Person\", \"Person\"),  # Prefix removal\n",
    "    (\"id\", \"idType\"),  # Reserved word\n",
    "]\n",
    "\n",
    "print(\"Testing sanitize_name():\")\n",
    "for input_name, expected in test_cases:\n",
    "    result = sanitize_name(input_name)\n",
    "    is_valid, error = validate_name(result)\n",
    "    status = \"✓\" if is_valid else f\"✗ {error}\"\n",
    "    print(f\"  '{input_name}' → '{result}' [{status}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2619810",
   "metadata": {},
   "source": [
    "## Helper Functions - Datatype Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d257ae3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_to_ontology_datatype(rdf_datatype: str) -> str:\n",
    "    \"\"\"\n",
    "    Map RDF/XSD datatypes to Fabric Ontology types.\n",
    "    \n",
    "    Fabric Ontology supports: String, Boolean, DateTime, Object, BigInt, Double\n",
    "    Note: Integer/Long/Int32/Int64 are NOT valid - use BigInt instead\n",
    "    \"\"\"\n",
    "    if rdf_datatype is None:\n",
    "        return \"String\"  # Default to String\n",
    "    \n",
    "    datatype_lower = rdf_datatype.lower()\n",
    "    \n",
    "    # String types\n",
    "    if any(t in datatype_lower for t in [\"string\", \"langstring\", \"anyuri\", \"token\", \"normalizedstring\", \"literal\"]):\n",
    "        return \"String\"\n",
    "    \n",
    "    # Integer/Long types - all map to BigInt (Fabric's only integer type)\n",
    "    if any(t in datatype_lower for t in [\"int\", \"integer\", \"long\", \"short\", \"byte\", \"unsignedshort\", \"unsignedbyte\", \"unsignedint\", \"unsignedlong\", \"nonpositiveinteger\", \"nonnegativeinteger\", \"positiveinteger\", \"negativeinteger\"]):\n",
    "        return \"BigInt\"\n",
    "    \n",
    "    # Double/Float types\n",
    "    if any(t in datatype_lower for t in [\"double\", \"float\", \"decimal\"]):\n",
    "        return \"Double\"\n",
    "    \n",
    "    # Boolean\n",
    "    if \"boolean\" in datatype_lower:\n",
    "        return \"Boolean\"\n",
    "    \n",
    "    # DateTime types\n",
    "    if any(t in datatype_lower for t in [\"datetime\", \"date\", \"time\", \"gyear\", \"gmonth\", \"gday\"]):\n",
    "        return \"DateTime\"\n",
    "    \n",
    "    # Default to String for unknown types\n",
    "    return \"String\"\n",
    "\n",
    "\n",
    "# Test datatype mapping\n",
    "test_types = [\n",
    "    (\"xsd:string\", \"String\"),\n",
    "    (\"xsd:integer\", \"BigInt\"),\n",
    "    (\"xsd:int\", \"BigInt\"),\n",
    "    (\"xsd:long\", \"BigInt\"),\n",
    "    (\"xsd:double\", \"Double\"),\n",
    "    (\"xsd:boolean\", \"Boolean\"),\n",
    "    (\"xsd:dateTime\", \"DateTime\"),\n",
    "    (\"http://www.w3.org/2001/XMLSchema#string\", \"String\"),\n",
    "    (None, \"String\"),\n",
    "]\n",
    "\n",
    "print(\"Testing map_to_ontology_datatype():\")\n",
    "for rdf_type, expected in test_types:\n",
    "    result = map_to_ontology_datatype(rdf_type)\n",
    "    status = \"✓\" if result == expected else f\"✗ expected {expected}\"\n",
    "    print(f\"  {rdf_type} → {result} [{status}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4655f8",
   "metadata": {},
   "source": [
    "## Helper Functions - Base64 Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e11febb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_payload(data: dict) -> str:\n",
    "    \"\"\"\n",
    "    Encode a dictionary as base64 JSON string for Fabric API.\n",
    "    \"\"\"\n",
    "    json_str = json.dumps(data, indent=2)\n",
    "    return base64.b64encode(json_str.encode('utf-8')).decode('utf-8')\n",
    "\n",
    "\n",
    "def decode_payload(encoded: str) -> dict:\n",
    "    \"\"\"\n",
    "    Decode a base64 JSON string back to a dictionary.\n",
    "    \"\"\"\n",
    "    json_str = base64.b64decode(encoded.encode('utf-8')).decode('utf-8')\n",
    "    return json.loads(json_str)\n",
    "\n",
    "\n",
    "# Test encoding/decoding\n",
    "test_data = {\"name\": \"Test\", \"value\": 123}\n",
    "encoded = encode_payload(test_data)\n",
    "decoded = decode_payload(encoded)\n",
    "print(f\"Encoding test: {test_data} → '{encoded[:30]}...'\")\n",
    "print(f\"Decode matches original: {decoded == test_data}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4f514c",
   "metadata": {},
   "source": [
    "## Load Schema Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d4788b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load node types\n",
    "df_node_types = spark.table(\"silver_node_types\")\n",
    "print(f\"Loaded {df_node_types.count()} node types\")\n",
    "df_node_types.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f403875e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load properties\n",
    "df_properties = spark.table(\"silver_properties\")\n",
    "print(f\"Loaded {df_properties.count()} properties\")\n",
    "df_properties.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4f66ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check property_type distribution first\n",
    "print(\"=== Property Type Distribution ===\")\n",
    "df_properties.groupBy(\"property_type\").count().orderBy(\"count\", ascending=False).show(truncate=False)\n",
    "\n",
    "# OWL property type URIs\n",
    "OWL_DATATYPE_PROPERTY = \"http://www.w3.org/2002/07/owl#DatatypeProperty\"\n",
    "OWL_OBJECT_PROPERTY = \"http://www.w3.org/2002/07/owl#ObjectProperty\"\n",
    "\n",
    "# Separate datatype and object properties using full OWL URIs\n",
    "df_datatype_props = df_properties.filter(\n",
    "    (F.col(\"property_type\") == OWL_DATATYPE_PROPERTY) | \n",
    "    (F.col(\"property_type\") == \"datatype\")  # Also support simple value for compatibility\n",
    ")\n",
    "df_object_props = df_properties.filter(\n",
    "    (F.col(\"property_type\") == OWL_OBJECT_PROPERTY) | \n",
    "    (F.col(\"property_type\") == \"object\")  # Also support simple value for compatibility\n",
    ")\n",
    "\n",
    "datatype_count = df_datatype_props.count()\n",
    "object_count = df_object_props.count()\n",
    "\n",
    "print(f\"\\nDatatype properties (become entity properties): {datatype_count}\")\n",
    "print(f\"Object properties (become relationship types): {object_count}\")\n",
    "\n",
    "if object_count == 0:\n",
    "    print(\"\\n⚠ WARNING: No object properties found!\")\n",
    "else:\n",
    "    print(f\"\\n✓ Found {object_count} object properties\")\n",
    "    print(\"\\nSample object properties (first 5):\")\n",
    "    df_object_props.select(\"property_name\", \"property_uri\", \"source_types\", \"target_types\").show(5, truncate=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20594c5a",
   "metadata": {},
   "source": [
    "## Build Entity Type Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1424d21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect node types to build entity types\n",
    "node_types = df_node_types.select(\n",
    "    \"node_type\", \"class_uri\", \"display_name\", \"description\"\n",
    ").collect()\n",
    "\n",
    "# Build lookup for node_type to entity_id mapping (for relationships)\n",
    "# Index by multiple variants to handle case mismatches between silver tables\n",
    "node_type_to_entity_id = {}\n",
    "for row in node_types:\n",
    "    entity_id = generate_id(f\"entity_{row['class_uri']}\")\n",
    "    node_type = row['node_type']\n",
    "    class_uri = row['class_uri']\n",
    "    \n",
    "    # Index by original values\n",
    "    node_type_to_entity_id[node_type] = entity_id\n",
    "    node_type_to_entity_id[class_uri] = entity_id\n",
    "    \n",
    "    # Also index by case variations to handle mismatches\n",
    "    node_type_to_entity_id[node_type.lower()] = entity_id\n",
    "    node_type_to_entity_id[node_type.capitalize()] = entity_id\n",
    "    node_type_to_entity_id[node_type.title()] = entity_id\n",
    "    \n",
    "    # Extract local name from URI and index by that too\n",
    "    if '#' in class_uri:\n",
    "        local_name = class_uri.split('#')[-1]\n",
    "        node_type_to_entity_id[local_name] = entity_id\n",
    "        node_type_to_entity_id[local_name.lower()] = entity_id\n",
    "        node_type_to_entity_id[local_name.capitalize()] = entity_id\n",
    "\n",
    "# Collect datatype properties with their domains\n",
    "datatype_props = df_datatype_props.select(\n",
    "    \"property_name\", \"property_uri\", \"data_type\", \"source_types\"\n",
    ").collect()\n",
    "\n",
    "print(f\"Building entity type definitions for {len(node_types)} types\")\n",
    "print(f\"Using {len(datatype_props)} datatype properties\")\n",
    "print(f\"Entity ID lookup has {len(node_type_to_entity_id)} keys (including case variants)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f1a136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build entity type definitions\n",
    "entity_types = []\n",
    "\n",
    "for node_row in node_types:\n",
    "    node_type = node_row[\"node_type\"]\n",
    "    class_uri = node_row[\"class_uri\"]\n",
    "    \n",
    "    # Generate entity ID\n",
    "    entity_id = node_type_to_entity_id[node_type]\n",
    "    \n",
    "    # Sanitize entity name\n",
    "    entity_name = sanitize_name(node_type)\n",
    "    \n",
    "    # Build properties list\n",
    "    # Start with required 'uri' property for the original RDF IRI\n",
    "    uri_prop_id = generate_id(f\"prop_{class_uri}_uri\")\n",
    "    properties = [\n",
    "        {\n",
    "            \"id\": uri_prop_id,\n",
    "            \"name\": \"uri\",\n",
    "            \"redefines\": None,\n",
    "            \"baseTypeNamespaceType\": None,\n",
    "            \"valueType\": \"String\"  # Must be String for entityIdParts\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Find datatype properties for this entity type\n",
    "    seen_prop_names = {\"uri\"}\n",
    "    \n",
    "    for prop_row in datatype_props:\n",
    "        source_types = prop_row[\"source_types\"] or []\n",
    "        \n",
    "        # Check if this property applies to this entity\n",
    "        if node_type in source_types or class_uri in source_types:\n",
    "            prop_name = sanitize_name(prop_row[\"property_name\"])\n",
    "            prop_uri = prop_row[\"property_uri\"]\n",
    "            \n",
    "            # Avoid duplicate property names\n",
    "            if prop_name not in seen_prop_names:\n",
    "                prop_id = generate_id(f\"prop_{class_uri}_{prop_uri}\")\n",
    "                prop_type = map_to_ontology_datatype(prop_row[\"data_type\"])\n",
    "                \n",
    "                properties.append({\n",
    "                    \"id\": prop_id,\n",
    "                    \"name\": prop_name,\n",
    "                    \"redefines\": None,\n",
    "                    \"baseTypeNamespaceType\": None,\n",
    "                    \"valueType\": prop_type\n",
    "                })\n",
    "                seen_prop_names.add(prop_name)\n",
    "    \n",
    "    # Build entity type definition per Fabric Ontology spec\n",
    "    entity_definition = {\n",
    "        \"id\": entity_id,\n",
    "        \"namespace\": ONTOLOGY_NAMESPACE,\n",
    "        \"baseEntityTypeId\": None,\n",
    "        \"name\": entity_name,\n",
    "        \"entityIdParts\": [uri_prop_id],  # Use URI as entity key\n",
    "        \"displayNamePropertyId\": uri_prop_id,  # Use URI as display name\n",
    "        \"namespaceType\": \"Custom\",  # Required\n",
    "        \"visibility\": \"Visible\",    # Required for UI\n",
    "        \"properties\": properties,\n",
    "        \"timeseriesProperties\": []  # Required field, empty for non-timeseries\n",
    "    }\n",
    "    \n",
    "    entity_types.append({\n",
    "        \"id\": entity_id,\n",
    "        \"name\": entity_name,\n",
    "        \"class_uri\": class_uri,\n",
    "        \"definition\": entity_definition\n",
    "    })\n",
    "\n",
    "print(f\"\\nBuilt {len(entity_types)} entity type definitions:\")\n",
    "for et in entity_types:\n",
    "    prop_count = len(et['definition']['properties'])\n",
    "    print(f\"  {et['name']}: {prop_count} properties\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3bca1a",
   "metadata": {},
   "source": [
    "## Build Relationship Type Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4c0c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect object properties (these become relationship types)\n",
    "object_props = df_object_props.select(\n",
    "    \"property_name\", \"property_uri\", \"source_types\", \"target_types\"\n",
    ").collect()\n",
    "\n",
    "print(f\"Building relationship type definitions from {len(object_props)} object properties\")\n",
    "\n",
    "# Debug: Check what we have\n",
    "print(f\"\\n--- Debug: Object Properties Analysis ---\")\n",
    "print(f\"Total object properties: {len(object_props)}\")\n",
    "\n",
    "if len(object_props) == 0:\n",
    "    print(\"\\n⚠ No object properties found!\")\n",
    "else:\n",
    "    print(f\"\\nFirst 3 object properties:\")\n",
    "    for i, row in enumerate(object_props[:3]):\n",
    "        print(f\"  {i+1}. {row['property_name']}\")\n",
    "        print(f\"     URI: {row['property_uri']}\")\n",
    "        print(f\"     source_types: {row['source_types']}\")\n",
    "        print(f\"     target_types: {row['target_types']}\")\n",
    "    \n",
    "    print(f\"\\n--- Debug: Entity ID lookup keys ---\")\n",
    "    print(f\"node_type_to_entity_id has {len(node_type_to_entity_id)} keys\")\n",
    "\n",
    "# Diagnostic: Check which target types are missing from silver_node_types\n",
    "all_targets = set()\n",
    "all_sources = set()\n",
    "for prop_row in object_props:\n",
    "    targets = prop_row[\"target_types\"] or []\n",
    "    sources = prop_row[\"source_types\"] or []\n",
    "    all_targets.update(targets)\n",
    "    all_sources.update(sources)\n",
    "\n",
    "def check_type_exists(type_name):\n",
    "    \"\"\"Check if a type exists in the lookup.\"\"\"\n",
    "    for variant in [type_name, type_name.lower(), type_name.capitalize(), type_name.title(), type_name.upper()]:\n",
    "        if variant in node_type_to_entity_id:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "missing_targets = [t for t in all_targets if not check_type_exists(t)]\n",
    "missing_sources = [s for s in all_sources if not check_type_exists(s)]\n",
    "\n",
    "print(f\"\\n=== Type Resolution Summary ===\")\n",
    "print(f\"Source types referenced: {len(all_sources)} - Missing: {len(missing_sources)}\")\n",
    "print(f\"Target types referenced: {len(all_targets)} - Missing: {len(missing_targets)}\")\n",
    "\n",
    "if missing_targets:\n",
    "    print(f\"\\n⚠ Missing target types (relationships will be skipped): {missing_targets}\")\n",
    "if missing_sources:\n",
    "    print(f\"⚠ Missing source types: {missing_sources}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad15917b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build relationship type definitions\n",
    "relationship_types = []\n",
    "\n",
    "# Track entity names for lookup\n",
    "entity_name_to_id = {et['name']: et['id'] for et in entity_types}\n",
    "node_type_to_entity_name = {et['class_uri']: et['name'] for et in entity_types}\n",
    "for et in entity_types:\n",
    "    # Also map by sanitized node_type name\n",
    "    node_type_to_entity_name[et['name']] = et['name']\n",
    "\n",
    "def lookup_entity_id(type_name: str) -> str:\n",
    "    \"\"\"Look up entity ID with fallback to case variations.\"\"\"\n",
    "    if type_name in node_type_to_entity_id:\n",
    "        return node_type_to_entity_id[type_name]\n",
    "    # Try case variations\n",
    "    for variant in [type_name.lower(), type_name.capitalize(), type_name.title(), type_name.upper()]:\n",
    "        if variant in node_type_to_entity_id:\n",
    "            return node_type_to_entity_id[variant]\n",
    "    return None\n",
    "\n",
    "for prop_row in object_props:\n",
    "    prop_name = prop_row[\"property_name\"]\n",
    "    prop_uri = prop_row[\"property_uri\"]\n",
    "    source_types = prop_row[\"source_types\"] or []\n",
    "    target_types = prop_row[\"target_types\"] or []\n",
    "    \n",
    "    # Sanitize relationship name\n",
    "    rel_name = sanitize_name(prop_name)\n",
    "    \n",
    "    # Find source entity types with case-insensitive lookup\n",
    "    sources = []\n",
    "    for st in source_types:\n",
    "        entity_id = lookup_entity_id(st)\n",
    "        if entity_id:\n",
    "            sources.append(entity_id)\n",
    "    \n",
    "    # Find target entity types with case-insensitive lookup\n",
    "    targets = []\n",
    "    for tt in target_types:\n",
    "        entity_id = lookup_entity_id(tt)\n",
    "        if entity_id:\n",
    "            targets.append(entity_id)\n",
    "    \n",
    "    # Skip if we can't resolve source or target\n",
    "    if not sources or not targets:\n",
    "        print(f\"  Warning: Skipping '{rel_name}' - unresolved source/target\")\n",
    "        print(f\"    Sources: {source_types} → {sources}\")\n",
    "        print(f\"    Targets: {target_types} → {targets}\")\n",
    "        continue\n",
    "    \n",
    "    # Fabric Ontology requires exactly ONE source and ONE target per relationship type.\n",
    "    # If RDF has multiple source→target combinations, create separate relationship types.\n",
    "    seen_combinations = set()\n",
    "    for source_id in set(sources):\n",
    "        for target_id in set(targets):\n",
    "            combo_key = f\"{source_id}_{target_id}\"\n",
    "            if combo_key in seen_combinations:\n",
    "                continue\n",
    "            seen_combinations.add(combo_key)\n",
    "            \n",
    "            # Generate unique relationship ID per source→target combination\n",
    "            rel_id = generate_id(f\"rel_{prop_uri}_{source_id}_{target_id}\")\n",
    "            \n",
    "            # Build relationship definition per Fabric Ontology spec\n",
    "            relationship_definition = {\n",
    "                \"id\": rel_id,\n",
    "                \"namespace\": ONTOLOGY_NAMESPACE,\n",
    "                \"name\": rel_name,\n",
    "                \"namespaceType\": \"Custom\",\n",
    "                \"source\": {\n",
    "                    \"entityTypeId\": source_id\n",
    "                },\n",
    "                \"target\": {\n",
    "                    \"entityTypeId\": target_id\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            relationship_types.append({\n",
    "                \"id\": rel_id,\n",
    "                \"name\": rel_name,\n",
    "                \"property_uri\": prop_uri,\n",
    "                \"source_entity_id\": source_id,\n",
    "                \"target_entity_id\": target_id,\n",
    "                \"definition\": relationship_definition\n",
    "            })\n",
    "\n",
    "print(f\"\\nBuilt {len(relationship_types)} relationship type definitions:\")\n",
    "for rt in relationship_types[:10]:  # Show first 10\n",
    "    print(f\"  {rt['name']}: {rt['source_entity_id']} → {rt['target_entity_id']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa473a90",
   "metadata": {},
   "source": [
    "## Assemble Definition Parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d05baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assemble the full definition structure with base64-encoded parts\n",
    "definition_parts = []\n",
    "\n",
    "# Add .platform metadata (required)\n",
    "platform_metadata = {\n",
    "    \"metadata\": {\n",
    "        \"type\": \"Ontology\",\n",
    "        \"displayName\": \"RDF Translated Ontology\"\n",
    "    }\n",
    "}\n",
    "definition_parts.append({\n",
    "    \"path\": \".platform\",\n",
    "    \"payload\": encode_payload(platform_metadata),\n",
    "    \"payloadType\": \"InlineBase64\"\n",
    "})\n",
    "\n",
    "# Add root definition.json - MUST BE EMPTY per Fabric Ontology spec\n",
    "# Entity types are discovered via EntityTypes/{ID}/definition.json paths\n",
    "root_definition = {}\n",
    "definition_parts.append({\n",
    "    \"path\": \"definition.json\",\n",
    "    \"payload\": encode_payload(root_definition),\n",
    "    \"payloadType\": \"InlineBase64\"\n",
    "})\n",
    "\n",
    "print(f\"Added .platform and empty root definition.json\")\n",
    "print(f\"Will add {len(entity_types)} entity types, {len(relationship_types)} relationship types as separate parts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46bc9766",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add entity type definitions (deduplicate by ID to avoid duplicate paths)\n",
    "seen_entity_ids = set()\n",
    "duplicate_entity_count = 0\n",
    "for et in entity_types:\n",
    "    if et['id'] in seen_entity_ids:\n",
    "        duplicate_entity_count += 1\n",
    "        continue\n",
    "    seen_entity_ids.add(et['id'])\n",
    "    path = f\"EntityTypes/{et['id']}/definition.json\"\n",
    "    definition_parts.append({\n",
    "        \"path\": path,\n",
    "        \"payload\": encode_payload(et['definition']),\n",
    "        \"payloadType\": \"InlineBase64\"\n",
    "    })\n",
    "\n",
    "print(f\"Added {len(seen_entity_ids)} entity type definition parts\")\n",
    "if duplicate_entity_count > 0:\n",
    "    print(f\"  ⚠️ Skipped {duplicate_entity_count} duplicate entity type IDs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec92026f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add relationship type definitions (deduplicate by ID to avoid duplicate paths)\n",
    "seen_rel_ids = set()\n",
    "duplicate_rel_count = 0\n",
    "for rt in relationship_types:\n",
    "    if rt['id'] in seen_rel_ids:\n",
    "        duplicate_rel_count += 1\n",
    "        continue\n",
    "    seen_rel_ids.add(rt['id'])\n",
    "    path = f\"RelationshipTypes/{rt['id']}/definition.json\"\n",
    "    definition_parts.append({\n",
    "        \"path\": path,\n",
    "        \"payload\": encode_payload(rt['definition']),\n",
    "        \"payloadType\": \"InlineBase64\"\n",
    "    })\n",
    "\n",
    "print(f\"Added {len(seen_rel_ids)} relationship type definition parts\")\n",
    "if duplicate_rel_count > 0:\n",
    "    print(f\"  ⚠️ Skipped {duplicate_rel_count} duplicate relationship type IDs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093da951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of all parts\n",
    "print(f\"\\nTotal definition parts: {len(definition_parts)}\")\n",
    "print(\"\\nPart paths:\")\n",
    "for part in definition_parts[:15]:  # Show first 15\n",
    "    print(f\"  {part['path']}\")\n",
    "if len(definition_parts) > 15:\n",
    "    print(f\"  ... and {len(definition_parts) - 15} more\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7227d2d8",
   "metadata": {},
   "source": [
    "## Validate Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbe5726",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_ontology_definition(parts: list[dict]) -> tuple[bool, list[str]]:\n",
    "    \"\"\"\n",
    "    Validate the ontology definition structure per Fabric Ontology spec.\n",
    "    \n",
    "    Returns (is_valid, list of errors)\n",
    "    \"\"\"\n",
    "    errors = []\n",
    "    \n",
    "    # Check for duplicate paths\n",
    "    all_paths = [p['path'] for p in parts]\n",
    "    seen_paths = set()\n",
    "    for path in all_paths:\n",
    "        if path in seen_paths:\n",
    "            errors.append(f\"Duplicate path: {path}\")\n",
    "        seen_paths.add(path)\n",
    "    \n",
    "    # Check required files exist\n",
    "    paths = set(all_paths)\n",
    "    if '.platform' not in paths:\n",
    "        errors.append(\"Missing .platform metadata\")\n",
    "    if 'definition.json' not in paths:\n",
    "        errors.append(\"Missing definition.json\")\n",
    "    \n",
    "    # Validate each part\n",
    "    for part in parts:\n",
    "        try:\n",
    "            payload = decode_payload(part['payload'])\n",
    "            \n",
    "            # Validate root definition should be empty\n",
    "            if part['path'] == 'definition.json':\n",
    "                if payload != {}:\n",
    "                    errors.append(f\"Root definition.json should be empty {{}}, got: {payload}\")\n",
    "            \n",
    "            # Validate entity type definitions\n",
    "            if part['path'].startswith('EntityTypes/') and part['path'].endswith('/definition.json'):\n",
    "                required_fields = ['id', 'namespace', 'name', 'namespaceType', 'visibility', 'properties']\n",
    "                for field in required_fields:\n",
    "                    if field not in payload:\n",
    "                        errors.append(f\"Entity {part['path']}: missing {field}\")\n",
    "                \n",
    "                # Validate name\n",
    "                if 'name' in payload:\n",
    "                    is_valid, error = validate_name(payload['name'])\n",
    "                    if not is_valid:\n",
    "                        errors.append(f\"Entity {part['path']}: {error}\")\n",
    "                \n",
    "                # Validate properties\n",
    "                for i, prop in enumerate(payload.get('properties', [])):\n",
    "                    for pf in ['id', 'name', 'valueType']:\n",
    "                        if pf not in prop:\n",
    "                            errors.append(f\"Entity {part['path']}, property {i}: missing {pf}\")\n",
    "            \n",
    "            # Validate relationship type definitions\n",
    "            if part['path'].startswith('RelationshipTypes/') and part['path'].endswith('/definition.json'):\n",
    "                required_fields = ['id', 'namespace', 'name', 'namespaceType', 'source', 'target']\n",
    "                for field in required_fields:\n",
    "                    if field not in payload:\n",
    "                        errors.append(f\"Relationship {part['path']}: missing {field}\")\n",
    "                \n",
    "                # Validate source and target have entityTypeId\n",
    "                if 'source' in payload:\n",
    "                    if 'entityTypeId' not in payload['source']:\n",
    "                        errors.append(f\"Relationship {part['path']}: source missing entityTypeId\")\n",
    "                if 'target' in payload:\n",
    "                    if 'entityTypeId' not in payload['target']:\n",
    "                        errors.append(f\"Relationship {part['path']}: target missing entityTypeId\")\n",
    "                \n",
    "                # Validate name\n",
    "                if 'name' in payload:\n",
    "                    is_valid, error = validate_name(payload['name'])\n",
    "                    if not is_valid:\n",
    "                        errors.append(f\"Relationship {part['path']}: {error}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            errors.append(f\"Part {part['path']}: decode error - {e}\")\n",
    "    \n",
    "    return len(errors) == 0, errors\n",
    "\n",
    "\n",
    "# Validate\n",
    "is_valid, validation_errors = validate_ontology_definition(definition_parts)\n",
    "\n",
    "if is_valid:\n",
    "    print(\"✓ Ontology definition is valid\")\n",
    "else:\n",
    "    print(f\"✗ Ontology definition has {len(validation_errors)} errors:\")\n",
    "    for error in validation_errors[:20]:  # Show first 20\n",
    "        print(f\"  - {error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5aa18b",
   "metadata": {},
   "source": [
    "## Save Definition to Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c3fec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory if it doesn't exist\n",
    "import os\n",
    "\n",
    "# In Fabric, Files/ is a mounted path\n",
    "output_dir = \"/lakehouse/default/Files/ontology_definitions\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "print(f\"Output directory: {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aeae177",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the full definition (for API upload)\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "definition_filename = f\"ontology_definition_{timestamp}.json\"\n",
    "\n",
    "full_definition = {\n",
    "    \"metadata\": {\n",
    "        \"generated_at\": datetime.now().isoformat(),\n",
    "        \"generator\": \"07_ontology_definition_generator\",\n",
    "        \"version\": ONTOLOGY_VERSION,\n",
    "        \"entity_type_count\": len(entity_types),\n",
    "        \"relationship_type_count\": len(relationship_types)\n",
    "    },\n",
    "    \"definition\": {\n",
    "        \"parts\": definition_parts\n",
    "    }\n",
    "}\n",
    "\n",
    "definition_path = os.path.join(output_dir, definition_filename)\n",
    "with open(definition_path, 'w') as f:\n",
    "    json.dump(full_definition, f, indent=2)\n",
    "\n",
    "print(f\"Saved: {definition_filename}\")\n",
    "print(f\"  Entity types: {len(entity_types)}\")\n",
    "print(f\"  Relationship types: {len(relationship_types)}\")\n",
    "print(f\"  Definition parts: {len(definition_parts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac25125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also save human-readable versions for debugging\n",
    "\n",
    "# Entity types summary - include full property objects for data binding\n",
    "entity_summary = []\n",
    "for et in entity_types:\n",
    "    entity_summary.append({\n",
    "        \"id\": et['id'],\n",
    "        \"name\": et['name'],\n",
    "        \"class_uri\": et['class_uri'],\n",
    "        \"property_count\": len(et['definition']['properties']),\n",
    "        # Include full property objects with id, name, valueType for notebook 09\n",
    "        \"properties\": [\n",
    "            {\"id\": p['id'], \"name\": p['name'], \"valueType\": p['valueType']}\n",
    "            for p in et['definition']['properties']\n",
    "        ]\n",
    "    })\n",
    "\n",
    "entity_summary_path = os.path.join(output_dir, f\"entity_types_{timestamp}.json\")\n",
    "with open(entity_summary_path, 'w') as f:\n",
    "    json.dump(entity_summary, f, indent=2)\n",
    "\n",
    "# Relationship types summary\n",
    "rel_summary = []\n",
    "for rt in relationship_types:\n",
    "    rel_summary.append({\n",
    "        \"id\": rt['id'],\n",
    "        \"name\": rt['name'],\n",
    "        \"property_uri\": rt['property_uri'],\n",
    "        \"source_entity_id\": rt['source_entity_id'],\n",
    "        \"target_entity_id\": rt['target_entity_id']\n",
    "    })\n",
    "\n",
    "rel_summary_path = os.path.join(output_dir, f\"relationship_types_{timestamp}.json\")\n",
    "with open(rel_summary_path, 'w') as f:\n",
    "    json.dump(rel_summary, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be297fe6",
   "metadata": {},
   "source": [
    "## Summary & Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcfed9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"Ontology Definition Generation Complete\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nGenerated:\")\n",
    "print(f\"  - {len(entity_types)} entity types\")\n",
    "print(f\"  - {len(relationship_types)} relationship types\")\n",
    "print(f\"  - {len(definition_parts)} definition parts\")\n",
    "\n",
    "print(f\"\\nOutput files in {output_dir}:\")\n",
    "print(f\"  - {definition_filename} (API-ready definition)\")\n",
    "print(f\"  - entity_types_{timestamp}.json (human-readable)\")\n",
    "print(f\"  - relationship_types_{timestamp}.json (human-readable)\")\n",
    "\n",
    "print(f\"\\nValidation: {'✓ PASSED' if is_valid else '✗ FAILED'}\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"Next Steps:\")\n",
    "print(\"=\"*60)\n",
    "print(\"1. Run F5.2 (Fabric Ontology REST API Client) to upload definition\")\n",
    "print(\"2. Run F5.3 (Lakehouse Data Binding) to bind gold tables\")\n",
    "print(\"3. Query the materialized graph via Fabric Graph!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d11ac8",
   "metadata": {},
   "source": [
    "## Display Generated Definition (Sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921a3be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show first entity type definition (decoded) as example\n",
    "if entity_types:\n",
    "    print(\"Sample Entity Type Definition:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(json.dumps(entity_types[0]['definition'], indent=2))\n",
    "\n",
    "print()\n",
    "\n",
    "# Show first relationship type definition (decoded) as example\n",
    "if relationship_types:\n",
    "    print(\"Sample Relationship Type Definition:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(json.dumps(relationship_types[0]['definition'], indent=2))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
