{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50259005",
   "metadata": {},
   "source": [
    "# 09 - Lakehouse Data Binding\n",
    "\n",
    "**Epic:** F5 - Fabric Ontology Integration  \n",
    "**Feature:** F5.3 - Lakehouse Data Binding  \n",
    "**Priority:** P1\n",
    "\n",
    "## Purpose\n",
    "\n",
    "Generate and upload data bindings that connect Ontology entity types to Lakehouse Delta tables. This allows the Ontology to materialize as a queryable Graph.\n",
    "\n",
    "## Input\n",
    "\n",
    "- Ontology configuration from `Files/config/ontology_config.json`\n",
    "- Entity/relationship type definitions from `Files/ontology_definitions/`\n",
    "- Gold tables: `gold_nodes`, `gold_edges`\n",
    "\n",
    "## Output\n",
    "\n",
    "- Data bindings uploaded to Ontology via REST API\n",
    "- Ontology connected to Lakehouse data\n",
    "\n",
    "## Binding Types\n",
    "\n",
    "| Type | Source | Purpose |\n",
    "|------|--------|----------|\n",
    "| Static (NonTimeSeries) | gold_nodes | Entity instance data |\n",
    "| Relationship | gold_edges | Relationships between entities |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5304443c",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee8ad7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import base64\n",
    "import time\n",
    "import requests\n",
    "from datetime import datetime\n",
    "from typing import Optional, List, Dict\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6b37f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fabric notebookutils for authentication\n",
    "from notebookutils import mssparkutils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943ab508",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d8df44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fabric API configuration\n",
    "FABRIC_API_BASE = \"https://api.fabric.microsoft.com\"\n",
    "FABRIC_API_VERSION = \"v1\"\n",
    "\n",
    "# Paths\n",
    "DEFINITIONS_DIR = \"/lakehouse/default/Files/ontology_definitions\"\n",
    "CONFIG_DIR = \"/lakehouse/default/Files/config\"\n",
    "\n",
    "# Gold tables to bind\n",
    "NODES_TABLE = \"gold_nodes\"\n",
    "EDGES_TABLE = \"gold_edges\"\n",
    "\n",
    "# Retry configuration\n",
    "MAX_RETRIES = 3\n",
    "RETRY_DELAY_SECONDS = 5\n",
    "LRO_POLL_INTERVAL_SECONDS = 2\n",
    "LRO_MAX_WAIT_SECONDS = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c40a57",
   "metadata": {},
   "source": [
    "## Load Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f710edb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ontology configuration from previous notebook\n",
    "config_path = os.path.join(CONFIG_DIR, \"ontology_config.json\")\n",
    "\n",
    "if not os.path.exists(config_path):\n",
    "    raise FileNotFoundError(\n",
    "        f\"Ontology config not found at {config_path}. \"\n",
    "        \"Please run notebook 08 first.\"\n",
    "    )\n",
    "\n",
    "with open(config_path, 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "ontology_id = config[\"ontology_id\"]\n",
    "workspace_id = config[\"workspace_id\"]\n",
    "lakehouse_id = config[\"lakehouse_id\"]\n",
    "\n",
    "print(f\"Ontology ID: {ontology_id}\")\n",
    "print(f\"Workspace ID: {workspace_id}\")\n",
    "print(f\"Lakehouse ID: {lakehouse_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4dfb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load entity type definitions (human-readable format)\n",
    "def get_latest_file(prefix: str) -> str:\n",
    "    files = os.listdir(DEFINITIONS_DIR)\n",
    "    matching = [f for f in files if f.startswith(prefix) and f.endswith(\".json\")]\n",
    "    if not matching:\n",
    "        raise FileNotFoundError(f\"No files matching {prefix}*.json\")\n",
    "    matching.sort(reverse=True)\n",
    "    return os.path.join(DEFINITIONS_DIR, matching[0])\n",
    "\n",
    "entity_file = get_latest_file(\"entity_types_\")\n",
    "with open(entity_file, 'r') as f:\n",
    "    entity_types = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(entity_types)} entity types from {entity_file}\")\n",
    "\n",
    "# Load relationship type definitions\n",
    "rel_file = get_latest_file(\"relationship_types_\")\n",
    "with open(rel_file, 'r') as f:\n",
    "    relationship_types = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(relationship_types)} relationship types from {rel_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2b30db",
   "metadata": {},
   "source": [
    "## Authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c93466",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fabric_token() -> str:\n",
    "    \"\"\"Get Entra ID token for Fabric API.\"\"\"\n",
    "    return mssparkutils.credentials.getToken(\"https://api.fabric.microsoft.com\")\n",
    "\n",
    "def get_headers() -> dict:\n",
    "    \"\"\"Get HTTP headers with authorization token.\"\"\"\n",
    "    return {\n",
    "        \"Authorization\": f\"Bearer {get_fabric_token()}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "# Test authentication\n",
    "token = get_fabric_token()\n",
    "print(f\"Authentication OK (token length: {len(token)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf07fab5",
   "metadata": {},
   "source": [
    "## API Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ddf2daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def api_request(\n",
    "    method: str,\n",
    "    endpoint: str,\n",
    "    data: Optional[dict] = None,\n",
    "    params: Optional[dict] = None\n",
    ") -> requests.Response:\n",
    "    \"\"\"\n",
    "    Make an API request with retry logic.\n",
    "    \"\"\"\n",
    "    url = f\"{FABRIC_API_BASE}/{endpoint}\"\n",
    "    \n",
    "    for attempt in range(MAX_RETRIES):\n",
    "        try:\n",
    "            headers = get_headers()\n",
    "            response = requests.request(\n",
    "                method=method,\n",
    "                url=url,\n",
    "                headers=headers,\n",
    "                json=data,\n",
    "                params=params,\n",
    "                timeout=60\n",
    "            )\n",
    "            \n",
    "            if response.status_code == 429:\n",
    "                retry_after = int(response.headers.get(\"Retry-After\", RETRY_DELAY_SECONDS))\n",
    "                print(f\"Rate limited. Waiting {retry_after}s...\")\n",
    "                time.sleep(retry_after)\n",
    "                continue\n",
    "            \n",
    "            if response.status_code >= 500:\n",
    "                print(f\"Server error {response.status_code}. Retrying...\")\n",
    "                time.sleep(RETRY_DELAY_SECONDS)\n",
    "                continue\n",
    "            \n",
    "            return response\n",
    "            \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            if attempt < MAX_RETRIES - 1:\n",
    "                time.sleep(RETRY_DELAY_SECONDS)\n",
    "            else:\n",
    "                raise\n",
    "    \n",
    "    return response\n",
    "\n",
    "\n",
    "def wait_for_lro(operation_url: str) -> dict:\n",
    "    \"\"\"Wait for a long-running operation to complete.\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    while time.time() - start_time < LRO_MAX_WAIT_SECONDS:\n",
    "        headers = get_headers()\n",
    "        response = requests.get(operation_url, headers=headers, timeout=60)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            result = response.json()\n",
    "            status = result.get(\"status\", \"Unknown\")\n",
    "            \n",
    "            if status in [\"Succeeded\", \"Completed\"]:\n",
    "                return result\n",
    "            elif status in [\"Failed\", \"Cancelled\"]:\n",
    "                raise Exception(f\"Operation {status}: {result.get('error', {})}\")\n",
    "        \n",
    "        time.sleep(LRO_POLL_INTERVAL_SECONDS)\n",
    "    \n",
    "    raise TimeoutError(f\"Operation timed out after {LRO_MAX_WAIT_SECONDS}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13d917e",
   "metadata": {},
   "source": [
    "## Analyze Gold Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356de049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check gold_nodes table structure\n",
    "df_nodes = spark.table(NODES_TABLE)\n",
    "print(f\"gold_nodes schema:\")\n",
    "df_nodes.printSchema()\n",
    "\n",
    "node_count = df_nodes.count()\n",
    "print(f\"\\nTotal nodes: {node_count}\")\n",
    "\n",
    "# Get unique labels\n",
    "labels = df_nodes.select(F.explode(\"labels\").alias(\"label\")).distinct().collect()\n",
    "print(f\"Node labels: {[row['label'] for row in labels]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a560e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check gold_edges table structure\n",
    "df_edges = spark.table(EDGES_TABLE)\n",
    "print(f\"gold_edges schema:\")\n",
    "df_edges.printSchema()\n",
    "\n",
    "edge_count = df_edges.count()\n",
    "print(f\"\\nTotal edges: {edge_count}\")\n",
    "\n",
    "# Get unique edge types\n",
    "edge_types = df_edges.select(\"type\").distinct().collect()\n",
    "print(f\"Edge types: {[row['type'] for row in edge_types]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a05af56",
   "metadata": {},
   "source": [
    "## Generate Data Binding Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b99f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_entity_binding(\n",
    "    entity_type_id: str,\n",
    "    entity_type_name: str,\n",
    "    properties: List[Dict],\n",
    "    workspace_id: str,\n",
    "    lakehouse_id: str,\n",
    "    table_name: str\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Generate a data binding configuration for an entity type.\n",
    "    \n",
    "    Args:\n",
    "        entity_type_id: ID of the entity type\n",
    "        entity_type_name: Name of the entity type (used to filter gold_nodes)\n",
    "        properties: List of property definitions with 'id' and 'name'\n",
    "        workspace_id: Workspace ID\n",
    "        lakehouse_id: Lakehouse ID\n",
    "        table_name: Source table name\n",
    "    \n",
    "    Returns:\n",
    "        Data binding JSON structure\n",
    "    \"\"\"\n",
    "    import uuid\n",
    "    binding_id = str(uuid.uuid4()).replace(\"-\", \"\")[:13]\n",
    "    \n",
    "    # Map properties to columns in the gold_nodes table\n",
    "    # gold_nodes has: id, labels[], properties{}\n",
    "    # We need to map entity properties to columns in the source table\n",
    "    property_bindings = []\n",
    "    \n",
    "    for prop in properties:\n",
    "        prop_name = prop[\"name\"]\n",
    "        prop_id = prop[\"id\"]\n",
    "        \n",
    "        # The 'uri' property maps to 'id' column\n",
    "        if prop_name == \"uri\":\n",
    "            property_bindings.append({\n",
    "                \"sourceColumnName\": \"id\",\n",
    "                \"propertyId\": prop_id\n",
    "            })\n",
    "        else:\n",
    "            # Other properties come from the 'properties' map column\n",
    "            # Fabric requires property columns to be top-level\n",
    "            # This means we need to flatten the properties map first\n",
    "            # For now, skip non-uri properties\n",
    "            # TODO: Add support for flattened property columns\n",
    "            pass\n",
    "    \n",
    "    binding = {\n",
    "        \"id\": binding_id,\n",
    "        \"dataBindingConfiguration\": {\n",
    "            \"dataBindingType\": \"NonTimeSeries\",\n",
    "            \"propertyBindings\": property_bindings,\n",
    "            \"entityKeyPropertyId\": next(\n",
    "                (p[\"id\"] for p in properties if p[\"name\"] == \"uri\"), \n",
    "                properties[0][\"id\"] if properties else None\n",
    "            ),\n",
    "            \"displayNamePropertyId\": next(\n",
    "                (p[\"id\"] for p in properties if p[\"name\"] == \"uri\"),\n",
    "                properties[0][\"id\"] if properties else None\n",
    "            ),\n",
    "            \"dataSource\": {\n",
    "                \"workspaceId\": workspace_id,\n",
    "                \"lakehouseId\": lakehouse_id,\n",
    "                \"tableName\": table_name\n",
    "            },\n",
    "            # Filter to only this entity type's label\n",
    "            \"entityFilter\": f\"array_contains(labels, '{entity_type_name}')\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return binding\n",
    "\n",
    "\n",
    "# Generate bindings for each entity type\n",
    "entity_bindings = []\n",
    "\n",
    "for et in entity_types:\n",
    "    binding = generate_entity_binding(\n",
    "        entity_type_id=et[\"id\"],\n",
    "        entity_type_name=et[\"name\"],\n",
    "        properties=et.get(\"properties\", []),\n",
    "        workspace_id=workspace_id,\n",
    "        lakehouse_id=lakehouse_id,\n",
    "        table_name=NODES_TABLE\n",
    "    )\n",
    "    entity_bindings.append({\n",
    "        \"entity_type_id\": et[\"id\"],\n",
    "        \"entity_type_name\": et[\"name\"],\n",
    "        \"binding\": binding\n",
    "    })\n",
    "\n",
    "print(f\"Generated {len(entity_bindings)} entity type bindings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64678e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show sample binding\n",
    "if entity_bindings:\n",
    "    print(\"Sample entity binding:\")\n",
    "    print(json.dumps(entity_bindings[0], indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df06939",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_relationship_binding(\n",
    "    relationship_type_id: str,\n",
    "    relationship_type_name: str,\n",
    "    workspace_id: str,\n",
    "    lakehouse_id: str,\n",
    "    table_name: str\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Generate a relationship binding configuration.\n",
    "    \n",
    "    Args:\n",
    "        relationship_type_id: ID of the relationship type\n",
    "        relationship_type_name: Name (used to filter gold_edges)\n",
    "        workspace_id: Workspace ID\n",
    "        lakehouse_id: Lakehouse ID\n",
    "        table_name: Source table name (gold_edges)\n",
    "    \n",
    "    Returns:\n",
    "        Relationship binding JSON structure\n",
    "    \"\"\"\n",
    "    import uuid\n",
    "    binding_id = str(uuid.uuid4()).replace(\"-\", \"\")[:13]\n",
    "    \n",
    "    binding = {\n",
    "        \"id\": binding_id,\n",
    "        \"relationshipBindingConfiguration\": {\n",
    "            \"sourceEntityKeyColumn\": \"source_id\",\n",
    "            \"targetEntityKeyColumn\": \"target_id\",\n",
    "            \"dataSource\": {\n",
    "                \"workspaceId\": workspace_id,\n",
    "                \"lakehouseId\": lakehouse_id,\n",
    "                \"tableName\": table_name\n",
    "            },\n",
    "            # Filter to only this relationship type\n",
    "            \"relationshipFilter\": f\"type = '{relationship_type_name}'\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return binding\n",
    "\n",
    "\n",
    "# Generate bindings for each relationship type\n",
    "relationship_bindings = []\n",
    "\n",
    "for rt in relationship_types:\n",
    "    binding = generate_relationship_binding(\n",
    "        relationship_type_id=rt[\"id\"],\n",
    "        relationship_type_name=rt[\"name\"],\n",
    "        workspace_id=workspace_id,\n",
    "        lakehouse_id=lakehouse_id,\n",
    "        table_name=EDGES_TABLE\n",
    "    )\n",
    "    relationship_bindings.append({\n",
    "        \"relationship_type_id\": rt[\"id\"],\n",
    "        \"relationship_type_name\": rt[\"name\"],\n",
    "        \"binding\": binding\n",
    "    })\n",
    "\n",
    "print(f\"Generated {len(relationship_bindings)} relationship type bindings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87357c3",
   "metadata": {},
   "source": [
    "## Encode Bindings as Definition Parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3e1683",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_payload(data: dict) -> str:\n",
    "    \"\"\"Encode a dictionary as base64 JSON string.\"\"\"\n",
    "    json_str = json.dumps(data, indent=2)\n",
    "    return base64.b64encode(json_str.encode('utf-8')).decode('utf-8')\n",
    "\n",
    "\n",
    "# Create definition parts for data bindings\n",
    "binding_parts = []\n",
    "\n",
    "# Add entity type data bindings\n",
    "for eb in entity_bindings:\n",
    "    path = f\"EntityTypes/{eb['entity_type_id']}/DataBindings/{eb['binding']['id']}.json\"\n",
    "    binding_parts.append({\n",
    "        \"path\": path,\n",
    "        \"payload\": encode_payload(eb['binding']),\n",
    "        \"payloadType\": \"InlineBase64\"\n",
    "    })\n",
    "\n",
    "# Add relationship type data bindings\n",
    "for rb in relationship_bindings:\n",
    "    path = f\"RelationshipTypes/{rb['relationship_type_id']}/DataBindings/{rb['binding']['id']}.json\"\n",
    "    binding_parts.append({\n",
    "        \"path\": path,\n",
    "        \"payload\": encode_payload(rb['binding']),\n",
    "        \"payloadType\": \"InlineBase64\"\n",
    "    })\n",
    "\n",
    "print(f\"Total binding parts to upload: {len(binding_parts)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c94170d",
   "metadata": {},
   "source": [
    "## Upload Data Bindings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca258185",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_ontology_definition(workspace_id: str, ontology_id: str, definition_parts: list) -> dict:\n",
    "    \"\"\"\n",
    "    Update the ontology definition with data bindings.\n",
    "    \"\"\"\n",
    "    endpoint = f\"{FABRIC_API_VERSION}/workspaces/{workspace_id}/ontologies/{ontology_id}/updateDefinition\"\n",
    "    \n",
    "    data = {\n",
    "        \"definition\": {\n",
    "            \"parts\": definition_parts\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    response = api_request(\"POST\", endpoint, data=data)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        print(\"Definition updated successfully\")\n",
    "        return response.json()\n",
    "    elif response.status_code == 202:\n",
    "        operation_url = response.headers.get(\"Location\")\n",
    "        print(f\"Update is async. Polling...\")\n",
    "        if operation_url:\n",
    "            return wait_for_lro(operation_url)\n",
    "    \n",
    "    print(f\"Failed to update: {response.status_code}\")\n",
    "    print(response.text)\n",
    "    raise Exception(f\"Update failed: {response.status_code} - {response.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46a5026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the data binding parts\n",
    "print(f\"Uploading {len(binding_parts)} data binding definitions...\")\n",
    "\n",
    "try:\n",
    "    result = update_ontology_definition(workspace_id, ontology_id, binding_parts)\n",
    "    print(\"\\nData bindings uploaded successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nFailed to upload data bindings: {e}\")\n",
    "    print(\"\\nNote: Data binding may require:\")\n",
    "    print(\"  - OneLake security disabled on lakehouse\")\n",
    "    print(\"  - Managed Delta tables (not external)\")\n",
    "    print(\"  - Column mapping disabled on tables\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c80dde5",
   "metadata": {},
   "source": [
    "## Save Binding Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c05fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save binding configuration for reference\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "binding_config = {\n",
    "    \"created_at\": datetime.now().isoformat(),\n",
    "    \"ontology_id\": ontology_id,\n",
    "    \"workspace_id\": workspace_id,\n",
    "    \"lakehouse_id\": lakehouse_id,\n",
    "    \"nodes_table\": NODES_TABLE,\n",
    "    \"edges_table\": EDGES_TABLE,\n",
    "    \"entity_bindings_count\": len(entity_bindings),\n",
    "    \"relationship_bindings_count\": len(relationship_bindings),\n",
    "    \"entity_bindings\": entity_bindings,\n",
    "    \"relationship_bindings\": relationship_bindings\n",
    "}\n",
    "\n",
    "binding_config_path = os.path.join(DEFINITIONS_DIR, f\"data_bindings_{timestamp}.json\")\n",
    "with open(binding_config_path, 'w') as f:\n",
    "    json.dump(binding_config, f, indent=2)\n",
    "\n",
    "print(f\"Saved binding configuration to: {binding_config_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e0abd8",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98d67da",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"Lakehouse Data Binding Complete\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nOntology ID: {ontology_id}\")\n",
    "print(f\"Lakehouse ID: {lakehouse_id}\")\n",
    "print(f\"\\nEntity Type Bindings: {len(entity_bindings)}\")\n",
    "print(f\"Relationship Bindings: {len(relationship_bindings)}\")\n",
    "print(f\"\\nSource Tables:\")\n",
    "print(f\"  Nodes: {NODES_TABLE} ({node_count} rows)\")\n",
    "print(f\"  Edges: {EDGES_TABLE} ({edge_count} rows)\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"Next Steps:\")\n",
    "print(\"=\"*60)\n",
    "print(\"1. Go to Fabric portal → Ontology → View your ontology\")\n",
    "print(\"2. Verify entity types show data counts\")\n",
    "print(\"3. Query the materialized Graph!\")\n",
    "print(\"4. (Optional) Connect Data Agent for NL2Ontology queries\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
