{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50259005",
   "metadata": {},
   "source": [
    "# 09 - Lakehouse Data Binding\n",
    "\n",
    "**Epic:** F5 - Fabric Ontology Integration  \n",
    "**Feature:** F5.3 - Lakehouse Data Binding  \n",
    "**Priority:** P1\n",
    "\n",
    "## Purpose\n",
    "\n",
    "Generate and upload data bindings that connect Ontology entity types to Lakehouse Delta tables. This allows the Ontology to materialize as a queryable Graph.\n",
    "\n",
    "## Input\n",
    "\n",
    "- Ontology configuration from `Files/config/ontology_config.json`\n",
    "- Entity/relationship type definitions from `Files/ontology_definitions/`\n",
    "- Gold tables: `gold_nodes`, `gold_edges`\n",
    "\n",
    "## Output\n",
    "\n",
    "- Data bindings uploaded to Ontology via REST API\n",
    "- Ontology connected to Lakehouse data\n",
    "\n",
    "## Binding Types\n",
    "\n",
    "| Type | Source | Purpose |\n",
    "|------|--------|----------|\n",
    "| Static (NonTimeSeries) | gold_nodes | Entity instance data |\n",
    "| Relationship | gold_edges | Relationships between entities |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5304443c",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee8ad7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import base64\n",
    "import time\n",
    "import requests\n",
    "from datetime import datetime\n",
    "from typing import Optional, List, Dict\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6b37f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fabric notebookutils for authentication\n",
    "from notebookutils import mssparkutils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943ab508",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d8df44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fabric API configuration\n",
    "FABRIC_API_BASE = \"https://api.fabric.microsoft.com\"\n",
    "FABRIC_API_VERSION = \"v1\"\n",
    "\n",
    "# Paths\n",
    "DEFINITIONS_DIR = \"/lakehouse/default/Files/ontology_definitions\"\n",
    "CONFIG_DIR = \"/lakehouse/default/Files/config\"\n",
    "\n",
    "# Gold tables to bind\n",
    "NODES_TABLE = \"gold_nodes\"\n",
    "EDGES_TABLE = \"gold_edges\"\n",
    "\n",
    "# Retry configuration\n",
    "MAX_RETRIES = 3\n",
    "RETRY_DELAY_SECONDS = 5\n",
    "LRO_POLL_INTERVAL_SECONDS = 2\n",
    "LRO_MAX_WAIT_SECONDS = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c40a57",
   "metadata": {},
   "source": [
    "## Load Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f710edb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ontology configuration from previous notebook\n",
    "config_path = os.path.join(CONFIG_DIR, \"ontology_config.json\")\n",
    "\n",
    "if not os.path.exists(config_path):\n",
    "    raise FileNotFoundError(\n",
    "        f\"Ontology config not found at {config_path}. \"\n",
    "        \"Please run notebook 08 first.\"\n",
    "    )\n",
    "\n",
    "with open(config_path, 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "ontology_id = config[\"ontology_id\"]\n",
    "workspace_id = config[\"workspace_id\"]\n",
    "lakehouse_id = config[\"lakehouse_id\"]\n",
    "\n",
    "print(f\"Ontology ID: {ontology_id}\")\n",
    "print(f\"Workspace ID: {workspace_id}\")\n",
    "print(f\"Lakehouse ID: {lakehouse_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4dfb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load entity and relationship types from the UPLOADED ontology via API\n",
    "# This ensures IDs match exactly what Fabric has registered\n",
    "\n",
    "def get_ontology_definition_from_api(workspace_id: str, ontology_id: str) -> dict:\n",
    "    \"\"\"Fetch the current ontology definition from Fabric API.\"\"\"\n",
    "    token = mssparkutils.credentials.getToken(\"https://api.fabric.microsoft.com\")\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {token}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    \n",
    "    # Get definition (may be async)\n",
    "    url = f\"{FABRIC_API_BASE}/{FABRIC_API_VERSION}/workspaces/{workspace_id}/ontologies/{ontology_id}/getDefinition\"\n",
    "    response = requests.post(url, headers=headers, timeout=120)\n",
    "    \n",
    "    if response.status_code == 202:\n",
    "        # Long-running operation - poll for completion\n",
    "        operation_url = response.headers.get(\"Location\")\n",
    "        print(f\"  Async operation, polling...\")\n",
    "        max_wait = 120\n",
    "        start = time.time()\n",
    "        while time.time() - start < max_wait:\n",
    "            r = requests.get(operation_url, headers=headers, timeout=60)\n",
    "            if r.status_code == 200:\n",
    "                result = r.json()\n",
    "                status = result.get(\"status\")\n",
    "                if status in [\"Succeeded\", \"Completed\"]:\n",
    "                    print(f\"  LRO completed, fetching definition...\")\n",
    "                    # After LRO completes, re-call getDefinition - it should return 200 now\n",
    "                    final_response = requests.post(url, headers=headers, timeout=120)\n",
    "                    if final_response.status_code == 200:\n",
    "                        return final_response.json()\n",
    "                    else:\n",
    "                        print(f\"  Re-fetch returned {final_response.status_code}\")\n",
    "                        # Try to get it from the result\n",
    "                        if \"definition\" in result:\n",
    "                            return result\n",
    "                        raise Exception(f\"Could not get definition after LRO: {final_response.status_code}\")\n",
    "                elif status in [\"Failed\", \"Cancelled\"]:\n",
    "                    raise Exception(f\"Get definition failed: {result}\")\n",
    "            time.sleep(2)\n",
    "        raise TimeoutError(\"Get definition timed out\")\n",
    "    elif response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        raise Exception(f\"Get definition failed: {response.status_code} - {response.text}\")\n",
    "\n",
    "def parse_ontology_definition(api_response: dict) -> tuple:\n",
    "    \"\"\"Parse entity types and relationship types from API response.\"\"\"\n",
    "    import base64\n",
    "    \n",
    "    entity_types = []\n",
    "    relationship_types = []\n",
    "    \n",
    "    parts = api_response.get(\"definition\", {}).get(\"parts\", [])\n",
    "    \n",
    "    print(f\"  Found {len(parts)} parts in definition\")\n",
    "    \n",
    "    # Debug: show first few paths to understand format\n",
    "    sample_paths = [p.get(\"path\", \"\") for p in parts[:5]]\n",
    "    print(f\"  Sample paths: {sample_paths}\")\n",
    "    \n",
    "    for part in parts:\n",
    "        path = part.get(\"path\", \"\")\n",
    "        payload = part.get(\"payload\", \"\")\n",
    "        \n",
    "        try:\n",
    "            content = json.loads(base64.b64decode(payload).decode(\"utf-8\"))\n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "        # Entity type definition files: EntityTypes/{id}/definition.json\n",
    "        # Handle both with and without leading slash\n",
    "        if \"EntityTypes/\" in path and path.endswith(\"/definition.json\"):\n",
    "            # Extract entity type with properties - convert IDs to strings for consistency\n",
    "            entity_types.append({\n",
    "                \"id\": str(content.get(\"id\")),\n",
    "                \"name\": content.get(\"name\"),\n",
    "                \"namespace\": content.get(\"namespace\"),\n",
    "                \"properties\": [\n",
    "                    {**p, \"id\": str(p.get(\"id\"))} for p in content.get(\"properties\", [])\n",
    "                ]\n",
    "            })\n",
    "        \n",
    "        # Relationship type definition files: RelationshipTypes/{id}/definition.json\n",
    "        elif \"RelationshipTypes/\" in path and path.endswith(\"/definition.json\"):\n",
    "            source = content.get(\"source\", {})\n",
    "            target = content.get(\"target\", {})\n",
    "            relationship_types.append({\n",
    "                \"id\": str(content.get(\"id\")),\n",
    "                \"name\": content.get(\"name\"),\n",
    "                \"namespace\": content.get(\"namespace\"),\n",
    "                \"source\": {\"entityTypeId\": str(source.get(\"entityTypeId\"))} if source.get(\"entityTypeId\") else {},\n",
    "                \"target\": {\"entityTypeId\": str(target.get(\"entityTypeId\"))} if target.get(\"entityTypeId\") else {}\n",
    "            })\n",
    "    \n",
    "    return entity_types, relationship_types\n",
    "\n",
    "# Fetch and parse the ontology definition\n",
    "print(\"Fetching ontology definition from Fabric API...\")\n",
    "api_response = get_ontology_definition_from_api(workspace_id, ontology_id)\n",
    "\n",
    "# Debug: show response structure\n",
    "print(f\"  Response keys: {list(api_response.keys())}\")\n",
    "if \"definition\" in api_response:\n",
    "    print(f\"  Definition keys: {list(api_response['definition'].keys())}\")\n",
    "\n",
    "entity_types, relationship_types = parse_ontology_definition(api_response)\n",
    "\n",
    "print(f\"Loaded {len(entity_types)} entity types from ontology\")\n",
    "print(f\"Loaded {len(relationship_types)} relationship types from ontology\")\n",
    "\n",
    "# Show sample entity type\n",
    "if entity_types:\n",
    "    sample = entity_types[0]\n",
    "    print(f\"\\nSample entity type:\")\n",
    "    print(f\"  ID: {sample['id']}\")\n",
    "    print(f\"  Name: {sample['name']}\")\n",
    "    print(f\"  Properties: {len(sample.get('properties', []))} defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2b30db",
   "metadata": {},
   "source": [
    "## Authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c93466",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fabric_token() -> str:\n",
    "    \"\"\"Get Entra ID token for Fabric API.\"\"\"\n",
    "    return mssparkutils.credentials.getToken(\"https://api.fabric.microsoft.com\")\n",
    "\n",
    "def get_headers() -> dict:\n",
    "    \"\"\"Get HTTP headers with authorization token.\"\"\"\n",
    "    return {\n",
    "        \"Authorization\": f\"Bearer {get_fabric_token()}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "# Test authentication\n",
    "token = get_fabric_token()\n",
    "print(f\"Authentication OK (token length: {len(token)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf07fab5",
   "metadata": {},
   "source": [
    "## API Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ddf2daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def api_request(\n",
    "    method: str,\n",
    "    endpoint: str,\n",
    "    data: Optional[dict] = None,\n",
    "    params: Optional[dict] = None,\n",
    "    timeout: int = 60\n",
    ") -> requests.Response:\n",
    "    \"\"\"\n",
    "    Make an API request with retry logic and configurable timeout.\n",
    "    \n",
    "    Args:\n",
    "        timeout: Request timeout in seconds (default 60, use longer for large payloads)\n",
    "    \"\"\"\n",
    "    url = f\"{FABRIC_API_BASE}/{endpoint}\"\n",
    "    current_timeout = timeout\n",
    "    \n",
    "    for attempt in range(MAX_RETRIES):\n",
    "        try:\n",
    "            headers = get_headers()\n",
    "            response = requests.request(\n",
    "                method=method,\n",
    "                url=url,\n",
    "                headers=headers,\n",
    "                json=data,\n",
    "                params=params,\n",
    "                timeout=current_timeout\n",
    "            )\n",
    "            \n",
    "            if response.status_code == 429:\n",
    "                retry_after = int(response.headers.get(\"Retry-After\", RETRY_DELAY_SECONDS))\n",
    "                print(f\"Rate limited. Waiting {retry_after}s...\")\n",
    "                time.sleep(retry_after)\n",
    "                continue\n",
    "            \n",
    "            if response.status_code >= 500:\n",
    "                print(f\"Server error {response.status_code}. Retrying...\")\n",
    "                time.sleep(RETRY_DELAY_SECONDS)\n",
    "                continue\n",
    "            \n",
    "            return response\n",
    "            \n",
    "        except requests.exceptions.ReadTimeout:\n",
    "            # Double timeout on each retry for large payloads\n",
    "            current_timeout = min(current_timeout * 2, 600)  # Max 10 minutes\n",
    "            print(f\"Read timeout. Retrying with {current_timeout}s timeout...\")\n",
    "            if attempt >= MAX_RETRIES - 1:\n",
    "                raise\n",
    "            continue\n",
    "            \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            if attempt < MAX_RETRIES - 1:\n",
    "                time.sleep(RETRY_DELAY_SECONDS)\n",
    "            else:\n",
    "                raise\n",
    "    \n",
    "    return response\n",
    "\n",
    "\n",
    "def wait_for_lro(operation_url: str, max_wait: int = None) -> dict:\n",
    "    \"\"\"Wait for a long-running operation to complete.\"\"\"\n",
    "    max_wait = max_wait or LRO_MAX_WAIT_SECONDS\n",
    "    start_time = time.time()\n",
    "    \n",
    "    while time.time() - start_time < max_wait:\n",
    "        headers = get_headers()\n",
    "        response = requests.get(operation_url, headers=headers, timeout=120)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            result = response.json()\n",
    "            status = result.get(\"status\", \"Unknown\")\n",
    "            \n",
    "            if status in [\"Succeeded\", \"Completed\"]:\n",
    "                return result\n",
    "            elif status in [\"Failed\", \"Cancelled\"]:\n",
    "                raise Exception(f\"Operation {status}: {result.get('error', {})}\")\n",
    "            else:\n",
    "                print(f\"Operation status: {status}\")\n",
    "        \n",
    "        time.sleep(LRO_POLL_INTERVAL_SECONDS)\n",
    "    \n",
    "    raise TimeoutError(f\"Operation timed out after {max_wait}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13d917e",
   "metadata": {},
   "source": [
    "## Analyze Gold Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356de049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check gold_nodes table structure\n",
    "df_nodes = spark.table(NODES_TABLE)\n",
    "print(f\"gold_nodes schema:\")\n",
    "df_nodes.printSchema()\n",
    "\n",
    "node_count = df_nodes.count()\n",
    "print(f\"\\nTotal nodes: {node_count}\")\n",
    "\n",
    "# Get unique labels\n",
    "labels = df_nodes.select(F.explode(\"labels\").alias(\"label\")).distinct().collect()\n",
    "print(f\"Node labels: {[row['label'] for row in labels]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a560e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check gold_edges table structure\n",
    "df_edges = spark.table(EDGES_TABLE)\n",
    "print(f\"gold_edges schema:\")\n",
    "df_edges.printSchema()\n",
    "\n",
    "edge_count = df_edges.count()\n",
    "print(f\"\\nTotal edges: {edge_count}\")\n",
    "\n",
    "# Get unique edge types\n",
    "edge_types = df_edges.select(\"type\").distinct().collect()\n",
    "print(f\"Edge types: {[row['type'] for row in edge_types]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a05af56",
   "metadata": {},
   "source": [
    "## Generate Data Binding Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b99f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_entity_binding(\n",
    "    entity_type_id: str,\n",
    "    entity_type_name: str,\n",
    "    properties: List[Dict],\n",
    "    workspace_id: str,\n",
    "    lakehouse_id: str,\n",
    "    table_name: str\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Generate a data binding configuration for an entity type.\n",
    "    \n",
    "    Per Fabric Ontology spec, DataBinding format:\n",
    "    - dataBindingConfiguration.dataBindingType: \"NonTimeSeries\" or \"TimeSeries\"\n",
    "    - dataBindingConfiguration.propertyBindings[]: sourceColumnName + targetPropertyId\n",
    "    - dataBindingConfiguration.sourceTableProperties: sourceType, workspaceId, itemId, sourceTableName, sourceSchema\n",
    "    \"\"\"\n",
    "    import uuid\n",
    "    binding_id = str(uuid.uuid4())\n",
    "    \n",
    "    # Map properties to columns in the gold_nodes table\n",
    "    # gold_nodes has: id, labels[], properties{}\n",
    "    property_bindings = []\n",
    "    \n",
    "    for prop in properties:\n",
    "        prop_name = prop[\"name\"]\n",
    "        prop_id = prop[\"id\"]\n",
    "        \n",
    "        # The 'uri' property maps to 'id' column\n",
    "        if prop_name == \"uri\":\n",
    "            property_bindings.append({\n",
    "                \"sourceColumnName\": \"id\",\n",
    "                \"targetPropertyId\": prop_id  # Must be targetPropertyId, not propertyId\n",
    "            })\n",
    "        # Other properties would need flattened columns in the source table\n",
    "        # TODO: Create entity-specific views with flattened properties\n",
    "    \n",
    "    binding = {\n",
    "        \"id\": binding_id,\n",
    "        \"dataBindingConfiguration\": {\n",
    "            \"dataBindingType\": \"NonTimeSeries\",\n",
    "            \"propertyBindings\": property_bindings,\n",
    "            \"sourceTableProperties\": {\n",
    "                \"sourceType\": \"LakehouseTable\",\n",
    "                \"workspaceId\": workspace_id,\n",
    "                \"itemId\": lakehouse_id,  # itemId = Lakehouse artifact ID\n",
    "                \"sourceTableName\": table_name,\n",
    "                \"sourceSchema\": \"dbo\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return binding\n",
    "\n",
    "\n",
    "# Generate bindings for each entity type\n",
    "entity_bindings = []\n",
    "\n",
    "for et in entity_types:\n",
    "    binding = generate_entity_binding(\n",
    "        entity_type_id=et[\"id\"],\n",
    "        entity_type_name=et[\"name\"],\n",
    "        properties=et.get(\"properties\", []),\n",
    "        workspace_id=workspace_id,\n",
    "        lakehouse_id=lakehouse_id,\n",
    "        table_name=NODES_TABLE\n",
    "    )\n",
    "    entity_bindings.append({\n",
    "        \"entity_type_id\": et[\"id\"],\n",
    "        \"entity_type_name\": et[\"name\"],\n",
    "        \"binding\": binding\n",
    "    })\n",
    "\n",
    "print(f\"Generated {len(entity_bindings)} entity type bindings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64678e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show sample binding\n",
    "if entity_bindings:\n",
    "    print(\"Sample entity binding:\")\n",
    "    print(json.dumps(entity_bindings[0], indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df06939",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_relationship_contextualization(\n",
    "    relationship_type_id: str,\n",
    "    relationship_type_name: str,\n",
    "    source_entity_id: str,\n",
    "    target_entity_id: str,\n",
    "    source_uri_prop_id: str,\n",
    "    target_uri_prop_id: str,\n",
    "    workspace_id: str,\n",
    "    lakehouse_id: str,\n",
    "    table_name: str\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Generate a relationship contextualization configuration.\n",
    "    \n",
    "    Per Fabric Ontology spec, Contextualization format:\n",
    "    - dataBindingTable: source table info\n",
    "    - sourceKeyRefBindings[]: sourceColumnName + targetPropertyId (maps source entity key)\n",
    "    - targetKeyRefBindings[]: sourceColumnName + targetPropertyId (maps target entity key)\n",
    "    \"\"\"\n",
    "    import uuid\n",
    "    binding_id = str(uuid.uuid4())\n",
    "    \n",
    "    contextualization = {\n",
    "        \"id\": binding_id,\n",
    "        \"dataBindingTable\": {\n",
    "            \"sourceType\": \"LakehouseTable\",\n",
    "            \"workspaceId\": workspace_id,\n",
    "            \"itemId\": lakehouse_id,\n",
    "            \"sourceTableName\": table_name,\n",
    "            \"sourceSchema\": \"dbo\"\n",
    "        },\n",
    "        \"sourceKeyRefBindings\": [\n",
    "            {\n",
    "                \"sourceColumnName\": \"source_id\",  # gold_edges.source_id column\n",
    "                \"targetPropertyId\": source_uri_prop_id  # Maps to source entity's uri property\n",
    "            }\n",
    "        ],\n",
    "        \"targetKeyRefBindings\": [\n",
    "            {\n",
    "                \"sourceColumnName\": \"target_id\",  # gold_edges.target_id column\n",
    "                \"targetPropertyId\": target_uri_prop_id  # Maps to target entity's uri property\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    return contextualization\n",
    "\n",
    "\n",
    "# Build lookup for entity URI property IDs\n",
    "# Each entity's 'uri' property is needed for relationship key bindings\n",
    "entity_uri_props = {}\n",
    "for et in entity_types:\n",
    "    for prop in et.get(\"properties\", []):\n",
    "        if prop[\"name\"] == \"uri\":\n",
    "            entity_uri_props[et[\"id\"]] = prop[\"id\"]\n",
    "            break\n",
    "\n",
    "print(f\"Entity URI property lookup: {len(entity_uri_props)} entries\")\n",
    "\n",
    "# Generate contextualizations for each relationship type\n",
    "relationship_bindings = []\n",
    "\n",
    "for rt in relationship_types:\n",
    "    # Source and target are objects with entityTypeId from API\n",
    "    source_id = rt.get(\"source\", {}).get(\"entityTypeId\")\n",
    "    target_id = rt.get(\"target\", {}).get(\"entityTypeId\")\n",
    "    \n",
    "    # Get URI property IDs for source and target entities\n",
    "    source_uri_prop = entity_uri_props.get(source_id)\n",
    "    target_uri_prop = entity_uri_props.get(target_id)\n",
    "    \n",
    "    if not source_uri_prop or not target_uri_prop:\n",
    "        print(f\"  Warning: Skipping '{rt['name']}' - missing URI property for source or target\")\n",
    "        continue\n",
    "    \n",
    "    binding = generate_relationship_contextualization(\n",
    "        relationship_type_id=rt[\"id\"],\n",
    "        relationship_type_name=rt[\"name\"],\n",
    "        source_entity_id=source_id,\n",
    "        target_entity_id=target_id,\n",
    "        source_uri_prop_id=source_uri_prop,\n",
    "        target_uri_prop_id=target_uri_prop,\n",
    "        workspace_id=workspace_id,\n",
    "        lakehouse_id=lakehouse_id,\n",
    "        table_name=EDGES_TABLE\n",
    "    )\n",
    "    relationship_bindings.append({\n",
    "        \"relationship_type_id\": rt[\"id\"],\n",
    "        \"relationship_type_name\": rt[\"name\"],\n",
    "        \"binding\": binding\n",
    "    })\n",
    "\n",
    "print(f\"Generated {len(relationship_bindings)} relationship type contextualizations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87357c3",
   "metadata": {},
   "source": [
    "## Encode Bindings as Definition Parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3e1683",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_payload(data: dict) -> str:\n",
    "    \"\"\"Encode a dictionary as base64 JSON string.\"\"\"\n",
    "    json_str = json.dumps(data, indent=2)\n",
    "    return base64.b64encode(json_str.encode('utf-8')).decode('utf-8')\n",
    "\n",
    "\n",
    "# Create definition parts for data bindings\n",
    "# IMPORTANT: Must include .platform and definition.json in every updateDefinition call\n",
    "binding_parts = []\n",
    "\n",
    "# Add required .platform metadata\n",
    "platform_metadata = {\n",
    "    \"metadata\": {\n",
    "        \"type\": \"Ontology\",\n",
    "        \"displayName\": \"RDF Translated Ontology\"\n",
    "    }\n",
    "}\n",
    "binding_parts.append({\n",
    "    \"path\": \".platform\",\n",
    "    \"payload\": encode_payload(platform_metadata),\n",
    "    \"payloadType\": \"InlineBase64\"\n",
    "})\n",
    "\n",
    "# Add required empty definition.json (per Fabric Ontology spec)\n",
    "binding_parts.append({\n",
    "    \"path\": \"definition.json\",\n",
    "    \"payload\": encode_payload({}),\n",
    "    \"payloadType\": \"InlineBase64\"\n",
    "})\n",
    "\n",
    "# Add entity type data bindings\n",
    "for eb in entity_bindings:\n",
    "    path = f\"EntityTypes/{eb['entity_type_id']}/DataBindings/{eb['binding']['id']}.json\"\n",
    "    binding_parts.append({\n",
    "        \"path\": path,\n",
    "        \"payload\": encode_payload(eb['binding']),\n",
    "        \"payloadType\": \"InlineBase64\"\n",
    "    })\n",
    "\n",
    "# Add relationship type data bindings (note: per Fabric spec, relationships use Contextualizations, not DataBindings)\n",
    "for rb in relationship_bindings:\n",
    "    path = f\"RelationshipTypes/{rb['relationship_type_id']}/Contextualizations/{rb['binding']['id']}.json\"\n",
    "    binding_parts.append({\n",
    "        \"path\": path,\n",
    "        \"payload\": encode_payload(rb['binding']),\n",
    "        \"payloadType\": \"InlineBase64\"\n",
    "    })\n",
    "\n",
    "print(f\"Total binding parts to upload: {len(binding_parts)}\")\n",
    "print(f\"  - Base files: 2 (.platform, definition.json)\")\n",
    "print(f\"  - Entity bindings: {len(entity_bindings)}\")\n",
    "print(f\"  - Relationship bindings: {len(relationship_bindings)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c94170d",
   "metadata": {},
   "source": [
    "## Upload Data Bindings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca258185",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_ontology_definition(workspace_id: str, ontology_id: str, definition_parts: list) -> dict:\n",
    "    \"\"\"\n",
    "    Update the ontology definition with data bindings.\n",
    "    Uses longer timeout for large payloads.\n",
    "    \"\"\"\n",
    "    endpoint = f\"{FABRIC_API_VERSION}/workspaces/{workspace_id}/ontologies/{ontology_id}/updateDefinition\"\n",
    "    \n",
    "    data = {\n",
    "        \"definition\": {\n",
    "            \"parts\": definition_parts\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Calculate timeout based on number of parts (at least 5 minutes, 2s per part)\n",
    "    upload_timeout = max(300, len(definition_parts) * 2)\n",
    "    print(f\"Using {upload_timeout}s timeout for {len(definition_parts)} parts...\")\n",
    "    \n",
    "    response = api_request(\"POST\", endpoint, data=data, timeout=upload_timeout)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        print(\"Definition updated successfully\")\n",
    "        return response.json()\n",
    "    elif response.status_code == 202:\n",
    "        operation_url = response.headers.get(\"Location\")\n",
    "        print(f\"Update is async. Polling for completion...\")\n",
    "        if operation_url:\n",
    "            # Wait longer for LRO based on parts count\n",
    "            lro_timeout = max(600, len(definition_parts) * 3)\n",
    "            return wait_for_lro(operation_url, max_wait=lro_timeout)\n",
    "    \n",
    "    print(f\"Failed to update: {response.status_code}\")\n",
    "    print(response.text)\n",
    "    raise Exception(f\"Update failed: {response.status_code} - {response.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46a5026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the data binding parts\n",
    "print(f\"Uploading {len(binding_parts)} data binding definitions...\")\n",
    "\n",
    "try:\n",
    "    result = update_ontology_definition(workspace_id, ontology_id, binding_parts)\n",
    "    print(\"\\nData bindings uploaded successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nFailed to upload data bindings: {e}\")\n",
    "    print(\"\\nNote: Data binding may require:\")\n",
    "    print(\"  - OneLake security disabled on lakehouse\")\n",
    "    print(\"  - Managed Delta tables (not external)\")\n",
    "    print(\"  - Column mapping disabled on tables\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c80dde5",
   "metadata": {},
   "source": [
    "## Save Binding Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c05fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save binding configuration for reference\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "binding_config = {\n",
    "    \"created_at\": datetime.now().isoformat(),\n",
    "    \"ontology_id\": ontology_id,\n",
    "    \"workspace_id\": workspace_id,\n",
    "    \"lakehouse_id\": lakehouse_id,\n",
    "    \"nodes_table\": NODES_TABLE,\n",
    "    \"edges_table\": EDGES_TABLE,\n",
    "    \"entity_bindings_count\": len(entity_bindings),\n",
    "    \"relationship_bindings_count\": len(relationship_bindings),\n",
    "    \"entity_bindings\": entity_bindings,\n",
    "    \"relationship_bindings\": relationship_bindings\n",
    "}\n",
    "\n",
    "binding_config_path = os.path.join(DEFINITIONS_DIR, f\"data_bindings_{timestamp}.json\")\n",
    "with open(binding_config_path, 'w') as f:\n",
    "    json.dump(binding_config, f, indent=2)\n",
    "\n",
    "print(f\"Saved binding configuration to: {binding_config_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e0abd8",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98d67da",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"Lakehouse Data Binding Complete\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nOntology ID: {ontology_id}\")\n",
    "print(f\"Lakehouse ID: {lakehouse_id}\")\n",
    "print(f\"\\nEntity Type Bindings: {len(entity_bindings)}\")\n",
    "print(f\"Relationship Bindings: {len(relationship_bindings)}\")\n",
    "print(f\"\\nSource Tables:\")\n",
    "print(f\"  Nodes: {NODES_TABLE} ({node_count} rows)\")\n",
    "print(f\"  Edges: {EDGES_TABLE} ({edge_count} rows)\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"Next Steps:\")\n",
    "print(\"=\"*60)\n",
    "print(\"1. Go to Fabric portal → Ontology → View your ontology\")\n",
    "print(\"2. Verify entity types show data counts\")\n",
    "print(\"3. Query the materialized Graph!\")\n",
    "print(\"4. (Optional) Connect Data Agent for NL2Ontology queries\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
