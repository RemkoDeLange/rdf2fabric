{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff5ca036",
   "metadata": {},
   "source": [
    "# 07 - Graph Model JSON Generator\n",
    "\n",
    "**Epic:** F5 - Fabric Graph Integration  \n",
    "**Feature:** F5.1 - Graph Model JSON Generator  \n",
    "**Priority:** P0\n",
    "\n",
    "## Purpose\n",
    "\n",
    "Generate Fabric Graph Model JSON definition from translated schema. This JSON file defines the graph schema for import into Fabric Graph databases.\n",
    "\n",
    "## Input\n",
    "\n",
    "- `silver_node_types` - Node type definitions from RDF classes\n",
    "- `silver_properties` - Property definitions (datatype and object)\n",
    "\n",
    "## Output\n",
    "\n",
    "- Graph Model JSON file saved to `Files/graph_models/`\n",
    "\n",
    "## Key Features\n",
    "\n",
    "- Maps RDF datatypes to Fabric Graph types\n",
    "- Sanitizes names for Graph API compatibility\n",
    "- Validates JSON structure before saving"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e4a5f6",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3851500d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import os\n",
    "from datetime import datetime\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StringType"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6191c32e",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b788eabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph model configuration\n",
    "GRAPH_MODEL_NAME = \"RdfTranslatedGraph\"\n",
    "GRAPH_MODEL_VERSION = \"1.0\"\n",
    "\n",
    "# Output path (relative to lakehouse Files)\n",
    "OUTPUT_DIR = \"Files/graph_models\"\n",
    "\n",
    "# Reserved words that cannot be used as names in Fabric Graph\n",
    "RESERVED_WORDS = {\n",
    "    \"node\", \"edge\", \"graph\", \"vertex\", \"source\", \"target\",\n",
    "    \"id\", \"label\", \"type\", \"name\", \"properties\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d17933f",
   "metadata": {},
   "source": [
    "## Verify Required Tables Exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c799448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that required tables exist before proceeding (using Spark catalog)\n",
    "required_tables = [\"silver_node_types\", \"silver_properties\"]\n",
    "missing_tables = []\n",
    "\n",
    "for table in required_tables:\n",
    "    try:\n",
    "        # Try to read the table from Spark catalog\n",
    "        spark.table(table).limit(1)\n",
    "    except Exception:\n",
    "        missing_tables.append(table)\n",
    "\n",
    "if missing_tables:\n",
    "    print(\"ERROR: Required tables not found:\")\n",
    "    for t in missing_tables:\n",
    "        print(f\"  - {t}\")\n",
    "    print(\"\\nPlease run the following notebooks first:\")\n",
    "    print(\"  - 01_rdf_parser.ipynb (creates bronze_triples)\")\n",
    "    print(\"  - 02_schema_analyzer.ipynb (creates bronze_schema_analysis)\")\n",
    "    print(\"  - 03_class_mapper.ipynb (creates silver_node_types)\")\n",
    "    print(\"  - 04_property_mapper.ipynb (creates silver_properties)\")\n",
    "    raise RuntimeError(f\"Missing required tables: {missing_tables}\")\n",
    "else:\n",
    "    print(\"All required tables exist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b793ae",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28117dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_rdf_to_fabric_type(rdf_datatype: str) -> str:\n",
    "    \"\"\"\n",
    "    Map RDF/XSD datatypes to Fabric Graph types.\n",
    "    \n",
    "    Fabric Graph supports: string, int, double, boolean, datetime\n",
    "    \"\"\"\n",
    "    if rdf_datatype is None:\n",
    "        return \"string\"  # Default to string\n",
    "    \n",
    "    datatype_lower = rdf_datatype.lower()\n",
    "    \n",
    "    # String types\n",
    "    if any(t in datatype_lower for t in [\"string\", \"langstring\", \"anyuri\", \"token\", \"normalizedstring\"]):\n",
    "        return \"string\"\n",
    "    \n",
    "    # Integer types\n",
    "    if any(t in datatype_lower for t in [\"integer\", \"int\", \"long\", \"short\", \"byte\", \"nonpositiveinteger\", \"nonnegativeinteger\", \"positiveinteger\", \"negativeinteger\", \"unsignedlong\", \"unsignedint\", \"unsignedshort\", \"unsignedbyte\"]):\n",
    "        return \"int\"\n",
    "    \n",
    "    # Double/Float types\n",
    "    if any(t in datatype_lower for t in [\"double\", \"float\", \"decimal\"]):\n",
    "        return \"double\"\n",
    "    \n",
    "    # Boolean\n",
    "    if \"boolean\" in datatype_lower:\n",
    "        return \"boolean\"\n",
    "    \n",
    "    # DateTime types\n",
    "    if any(t in datatype_lower for t in [\"datetime\", \"date\", \"time\", \"gyear\", \"gmonth\", \"gday\"]):\n",
    "        return \"datetime\"\n",
    "    \n",
    "    # Default to string for unknown types\n",
    "    return \"string\"\n",
    "\n",
    "\n",
    "def sanitize_name(name: str) -> str:\n",
    "    \"\"\"\n",
    "    Sanitize a name for Fabric Graph compatibility.\n",
    "    \n",
    "    - Remove special characters\n",
    "    - Replace spaces with underscores\n",
    "    - Handle reserved words\n",
    "    - Ensure name starts with letter\n",
    "    \"\"\"\n",
    "    if name is None:\n",
    "        return \"Unknown\"\n",
    "    \n",
    "    # Remove namespace prefixes if present (take local name)\n",
    "    if \":\" in name and not name.startswith(\"http\"):\n",
    "        name = name.split(\":\")[-1]\n",
    "    \n",
    "    # Replace spaces and hyphens with underscores\n",
    "    name = re.sub(r\"[\\s-]+\", \"_\", name)\n",
    "    \n",
    "    # Remove special characters (keep only alphanumeric and underscore)\n",
    "    name = re.sub(r\"[^a-zA-Z0-9_]\", \"\", name)\n",
    "    \n",
    "    # Ensure starts with letter\n",
    "    if name and not name[0].isalpha():\n",
    "        name = \"N_\" + name\n",
    "    \n",
    "    # Handle reserved words\n",
    "    if name.lower() in RESERVED_WORDS:\n",
    "        name = name + \"_type\"\n",
    "    \n",
    "    return name if name else \"Unknown\"\n",
    "\n",
    "\n",
    "def validate_graph_model(model: dict) -> tuple[bool, list[str]]:\n",
    "    \"\"\"\n",
    "    Validate the graph model structure.\n",
    "    \n",
    "    Returns tuple of (is_valid, list of errors)\n",
    "    \"\"\"\n",
    "    errors = []\n",
    "    \n",
    "    # Check required top-level fields\n",
    "    if \"name\" not in model:\n",
    "        errors.append(\"Missing required field: name\")\n",
    "    if \"nodes\" not in model:\n",
    "        errors.append(\"Missing required field: nodes\")\n",
    "    if \"edges\" not in model:\n",
    "        errors.append(\"Missing required field: edges\")\n",
    "    \n",
    "    # Validate nodes\n",
    "    node_names = set()\n",
    "    for i, node in enumerate(model.get(\"nodes\", [])):\n",
    "        if \"name\" not in node:\n",
    "            errors.append(f\"Node {i}: missing name\")\n",
    "        else:\n",
    "            if node[\"name\"] in node_names:\n",
    "                errors.append(f\"Duplicate node name: {node['name']}\")\n",
    "            node_names.add(node[\"name\"])\n",
    "        \n",
    "        if \"properties\" not in node:\n",
    "            errors.append(f\"Node {node.get('name', i)}: missing properties array\")\n",
    "        else:\n",
    "            for j, prop in enumerate(node[\"properties\"]):\n",
    "                if \"name\" not in prop:\n",
    "                    errors.append(f\"Node {node.get('name', i)}, property {j}: missing name\")\n",
    "                if \"type\" not in prop:\n",
    "                    errors.append(f\"Node {node.get('name', i)}, property {j}: missing type\")\n",
    "    \n",
    "    # Validate edges\n",
    "    edge_names = set()\n",
    "    for i, edge in enumerate(model.get(\"edges\", [])):\n",
    "        if \"name\" not in edge:\n",
    "            errors.append(f\"Edge {i}: missing name\")\n",
    "        else:\n",
    "            if edge[\"name\"] in edge_names:\n",
    "                errors.append(f\"Duplicate edge name: {edge['name']}\")\n",
    "            edge_names.add(edge[\"name\"])\n",
    "        \n",
    "        if \"source\" not in edge:\n",
    "            errors.append(f\"Edge {edge.get('name', i)}: missing source\")\n",
    "        elif edge[\"source\"] not in node_names:\n",
    "            errors.append(f\"Edge {edge.get('name', i)}: source '{edge['source']}' not in node types\")\n",
    "        \n",
    "        if \"target\" not in edge:\n",
    "            errors.append(f\"Edge {edge.get('name', i)}: missing target\")\n",
    "        elif edge[\"target\"] not in node_names:\n",
    "            errors.append(f\"Edge {edge.get('name', i)}: target '{edge['target']}' not in node types\")\n",
    "    \n",
    "    return len(errors) == 0, errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14bd9e85",
   "metadata": {},
   "source": [
    "## Load Schema Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73dee03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load node types\n",
    "df_node_types = spark.table(\"silver_node_types\")\n",
    "print(f\"Loaded {df_node_types.count()} node types\")\n",
    "df_node_types.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63f665e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load properties\n",
    "df_properties = spark.table(\"silver_properties\")\n",
    "print(f\"Loaded {df_properties.count()} properties\")\n",
    "df_properties.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a933c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate datatype and object properties\n",
    "df_datatype_props = df_properties.filter(F.col(\"property_type\") == \"datatype\")\n",
    "df_object_props = df_properties.filter(F.col(\"property_type\") == \"object\")\n",
    "\n",
    "print(f\"Datatype properties: {df_datatype_props.count()}\")\n",
    "print(f\"Object properties: {df_object_props.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f327f4bd",
   "metadata": {},
   "source": [
    "## Build Node Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d061c656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect node types\n",
    "node_types = df_node_types.select(\"node_type\", \"class_uri\").collect()\n",
    "node_type_set = {row[\"node_type\"] for row in node_types}\n",
    "\n",
    "# Collect datatype properties with their domains\n",
    "datatype_props = df_datatype_props.select(\n",
    "    \"property_name\", \"data_type\", \"source_types\"\n",
    ").collect()\n",
    "\n",
    "print(f\"Building definitions for {len(node_types)} node types\")\n",
    "print(f\"Using {len(datatype_props)} datatype properties\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1cb444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build node definitions with properties\n",
    "nodes = []\n",
    "\n",
    "for node_row in node_types:\n",
    "    node_type = node_row[\"node_type\"]\n",
    "    class_uri = node_row[\"class_uri\"]\n",
    "    \n",
    "    # Sanitize node name\n",
    "    node_name = sanitize_name(node_type)\n",
    "    \n",
    "    # Find properties for this node type\n",
    "    # Check if node type is in source_types array\n",
    "    node_properties = []\n",
    "    \n",
    "    for prop_row in datatype_props:\n",
    "        source_types = prop_row[\"source_types\"] or []\n",
    "        \n",
    "        # Check if this property applies to this node type\n",
    "        # Match either by exact node_type or by class_uri IRI\n",
    "        if node_type in source_types or class_uri in source_types:\n",
    "            prop_name = sanitize_name(prop_row[\"property_name\"])\n",
    "            prop_type = map_rdf_to_fabric_type(prop_row[\"data_type\"])\n",
    "            \n",
    "            # Avoid duplicate properties\n",
    "            if not any(p[\"name\"] == prop_name for p in node_properties):\n",
    "                node_properties.append({\n",
    "                    \"name\": prop_name,\n",
    "                    \"type\": prop_type\n",
    "                })\n",
    "    \n",
    "    # Add default 'uri' property for the original RDF IRI\n",
    "    node_properties.insert(0, {\"name\": \"uri\", \"type\": \"string\"})\n",
    "    \n",
    "    nodes.append({\n",
    "        \"name\": node_name,\n",
    "        \"properties\": node_properties\n",
    "    })\n",
    "\n",
    "print(f\"Built {len(nodes)} node definitions\")\n",
    "for node in nodes[:5]:  # Show first 5\n",
    "    print(f\"  {node['name']}: {len(node['properties'])} properties\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de56eed",
   "metadata": {},
   "source": [
    "## Build Edge Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8246f5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect object properties (these become edges)\n",
    "object_props = df_object_props.select(\n",
    "    \"property_name\", \"source_types\", \"target_types\"\n",
    ").collect()\n",
    "\n",
    "print(f\"Processing {len(object_props)} object properties for edge definitions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0c3d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build edge definitions\n",
    "edges = []\n",
    "seen_edges = set()  # Track unique (name, source, target) combinations\n",
    "\n",
    "# Get sanitized node names for lookup\n",
    "node_name_map = {sanitize_name(row[\"node_type\"]): sanitize_name(row[\"node_type\"]) for row in node_types}\n",
    "class_uri_to_node = {row[\"class_uri\"]: sanitize_name(row[\"node_type\"]) for row in node_types}\n",
    "\n",
    "for prop_row in object_props:\n",
    "    edge_name = sanitize_name(prop_row[\"property_name\"])\n",
    "    source_types = prop_row[\"source_types\"] or []\n",
    "    target_types = prop_row[\"target_types\"] or []\n",
    "    \n",
    "    # Skip if no targets specified\n",
    "    if not target_types:\n",
    "        continue\n",
    "    \n",
    "    # Create edge for each source-target combination\n",
    "    for source_type in source_types:\n",
    "        source = class_uri_to_node.get(source_type) or node_name_map.get(sanitize_name(source_type))\n",
    "        \n",
    "        if not source:\n",
    "            continue  # Skip if source not found\n",
    "        \n",
    "        for target_type in target_types:\n",
    "            target = class_uri_to_node.get(target_type) or node_name_map.get(sanitize_name(target_type))\n",
    "            \n",
    "            if not target:\n",
    "                continue  # Skip if target not found\n",
    "            \n",
    "            # Avoid duplicates\n",
    "            edge_key = (edge_name, source, target)\n",
    "            if edge_key in seen_edges:\n",
    "                continue\n",
    "            seen_edges.add(edge_key)\n",
    "            \n",
    "            edges.append({\n",
    "                \"name\": edge_name,\n",
    "                \"source\": source,\n",
    "                \"target\": target,\n",
    "                \"properties\": []\n",
    "            })\n",
    "\n",
    "print(f\"Built {len(edges)} edge definitions\")\n",
    "for edge in edges[:5]:  # Show first 5\n",
    "    print(f\"  {edge['source']} --[{edge['name']}]--> {edge['target']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3c93d3",
   "metadata": {},
   "source": [
    "## Assemble Graph Model JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dab5546",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the graph model\n",
    "graph_model = {\n",
    "    \"name\": GRAPH_MODEL_NAME,\n",
    "    \"version\": GRAPH_MODEL_VERSION,\n",
    "    \"created\": datetime.utcnow().isoformat() + \"Z\",\n",
    "    \"nodes\": nodes,\n",
    "    \"edges\": edges\n",
    "}\n",
    "\n",
    "# Display summary\n",
    "print(f\"Graph Model: {graph_model['name']} v{graph_model['version']}\")\n",
    "print(f\"  Node types: {len(graph_model['nodes'])}\")\n",
    "print(f\"  Edge types: {len(graph_model['edges'])}\")\n",
    "print(f\"  Total properties: {sum(len(n['properties']) for n in nodes)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc3bda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate the model\n",
    "is_valid, errors = validate_graph_model(graph_model)\n",
    "\n",
    "if is_valid:\n",
    "    print(\"Graph model validation passed\")\n",
    "else:\n",
    "    print(f\"Graph model validation failed with {len(errors)} errors:\")\n",
    "    for error in errors[:10]:  # Show first 10 errors\n",
    "        print(f\"  - {error}\")\n",
    "    if len(errors) > 10:\n",
    "        print(f\"  ... and {len(errors) - 10} more errors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f997faf",
   "metadata": {},
   "source": [
    "## Save Graph Model JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6547f734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory if needed\n",
    "# In Fabric, Files directory is at /lakehouse/default/Files\n",
    "output_base = \"/lakehouse/default/Files/graph_models\"\n",
    "os.makedirs(output_base, exist_ok=True)\n",
    "\n",
    "# Generate filename with timestamp\n",
    "timestamp = datetime.utcnow().strftime(\"%Y%m%d_%H%M%S\")\n",
    "filename = f\"graph_model_{GRAPH_MODEL_NAME}_{timestamp}.json\"\n",
    "output_path = os.path.join(output_base, filename)\n",
    "\n",
    "# Also save a \"latest\" version for easy access\n",
    "latest_path = os.path.join(output_base, f\"graph_model_{GRAPH_MODEL_NAME}_latest.json\")\n",
    "\n",
    "print(f\"Output paths:\")\n",
    "print(f\"  Timestamped: {output_path}\")\n",
    "print(f\"  Latest: {latest_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96197852",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the JSON file\n",
    "json_output = json.dumps(graph_model, indent=2)\n",
    "\n",
    "# Save timestamped version\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(json_output)\n",
    "print(f\"Saved timestamped model: {output_path}\")\n",
    "\n",
    "# Save latest version\n",
    "with open(latest_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(json_output)\n",
    "print(f\"Saved latest model: {latest_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d748f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the JSON output\n",
    "print(\"=\" * 60)\n",
    "print(\"GRAPH MODEL JSON PREVIEW\")\n",
    "print(\"=\" * 60)\n",
    "# Show condensed version for preview\n",
    "preview_model = {\n",
    "    \"name\": graph_model[\"name\"],\n",
    "    \"version\": graph_model[\"version\"],\n",
    "    \"created\": graph_model[\"created\"],\n",
    "    \"nodes\": graph_model[\"nodes\"][:3] if len(graph_model[\"nodes\"]) > 3 else graph_model[\"nodes\"],\n",
    "    \"edges\": graph_model[\"edges\"][:3] if len(graph_model[\"edges\"]) > 3 else graph_model[\"edges\"],\n",
    "    \"_truncated\": {\n",
    "        \"total_nodes\": len(graph_model[\"nodes\"]),\n",
    "        \"total_edges\": len(graph_model[\"edges\"])\n",
    "    }\n",
    "}\n",
    "print(json.dumps(preview_model, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc899d3b",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook generates a Fabric Graph Model JSON definition from the translated RDF schema.\n",
    "\n",
    "**Pipeline Position:** Step 7 (after gold layer is written)\n",
    "\n",
    "**Dependencies:**\n",
    "- `silver_node_types` - Node type definitions\n",
    "- `silver_properties` - Property definitions\n",
    "\n",
    "**Outputs:**\n",
    "- `Files/graph_models/graph_model_{name}_{timestamp}.json` - Timestamped version\n",
    "- `Files/graph_models/graph_model_{name}_latest.json` - Latest version for easy access\n",
    "\n",
    "**Next Steps:**\n",
    "- Use the Graph Model JSON with Fabric Graph API\n",
    "- Import gold_nodes and gold_edges data after creating graph schema"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
